{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### This notebook generates abstract topics from the large collection of review comments. Review comments are grouped by product id and preprocessed then fed to LDAmodel to get topics.\n",
    "#### To reproduce  the results, load the saved LDA model and run last 7 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the necessary packages\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from gensim import corpora, models\n",
    "from gensim.models import Phrases\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import operator\n",
    "import nltk\n",
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "from pprint import pprint\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_pickle('Clean_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter required columns\n",
    "df = df.filter(items=['asin','reviewText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00004R940</td>\n",
       "      <td>I'll admit it -- I'm a bit of a snob when it c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00004S4TZ</td>\n",
       "      <td>Maybe when Mom cooked for a big family every d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00004RDAZ</td>\n",
       "      <td>This was an impulse buy, I was looking for a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00004S576</td>\n",
       "      <td>I bought this rice cooker based on the first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00004SBIB</td>\n",
       "      <td>I went mad when I first saw this amazing cassa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053291</th>\n",
       "      <td>B01HCJCM52</td>\n",
       "      <td>These are exactly what I was looking for to so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053292</th>\n",
       "      <td>B01HIAZ9BY</td>\n",
       "      <td>Looks cheap but its sturdy to my surprise. //G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053293</th>\n",
       "      <td>B01HIAZ9BY</td>\n",
       "      <td>Great look and quality //Great product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053294</th>\n",
       "      <td>B01HHAW9HW</td>\n",
       "      <td>This is a great lunchbox. My daughter loves th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053295</th>\n",
       "      <td>B01HHHGFQ6</td>\n",
       "      <td>well made and comfortable, as advertised //Fiv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6053296 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               asin                                         reviewText\n",
       "0        B00004R940  I'll admit it -- I'm a bit of a snob when it c...\n",
       "1        B00004S4TZ  Maybe when Mom cooked for a big family every d...\n",
       "2        B00004RDAZ  This was an impulse buy, I was looking for a b...\n",
       "3        B00004S576  I bought this rice cooker based on the first t...\n",
       "4        B00004SBIB  I went mad when I first saw this amazing cassa...\n",
       "...             ...                                                ...\n",
       "6053291  B01HCJCM52  These are exactly what I was looking for to so...\n",
       "6053292  B01HIAZ9BY  Looks cheap but its sturdy to my surprise. //G...\n",
       "6053293  B01HIAZ9BY             Great look and quality //Great product\n",
       "6053294  B01HHAW9HW  This is a great lunchbox. My daughter loves th...\n",
       "6053295  B01HHHGFQ6  well made and comfortable, as advertised //Fiv...\n",
       "\n",
       "[6053296 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Products: 188138\n"
     ]
    }
   ],
   "source": [
    "#Check the no.of unique product id\n",
    "n_products = df.asin.unique().shape[0]\n",
    "print('Number of Products:', n_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group reviews by product id and add a column count\n",
    "df_grouped = df.groupby(['asin']).agg({'reviewText': ' ** '.join,'asin':'size'}).rename(columns={'asin':'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort dataframe by count in descending order\n",
    "df_grouped.sort_values(by = 'count',ascending=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "df_grouped.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00FLYWNYQ</td>\n",
       "      <td>I was excited to try this so as soon as I got ...</td>\n",
       "      <td>7433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00COK3FD8</td>\n",
       "      <td>The tupperware was exactly as described. I lov...</td>\n",
       "      <td>4563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B009HVH4XO</td>\n",
       "      <td>i have 4 of these and i didn't think they coul...</td>\n",
       "      <td>4460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00NX47YP4</td>\n",
       "      <td>Pros:\\nSeems to weigh accurately\\nEasy to use ...</td>\n",
       "      <td>4379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00902X68W</td>\n",
       "      <td>I love these sheets.  They are so soft and coz...</td>\n",
       "      <td>3725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188133</th>\n",
       "      <td>B00176NOZO</td>\n",
       "      <td>I'm not sure if the one I received was defecti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188134</th>\n",
       "      <td>B00KLFM0ZW</td>\n",
       "      <td>They are really nice quality and they look rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188135</th>\n",
       "      <td>B00FYHDHT0</td>\n",
       "      <td>Luxury look at a reasonable price.  Very elega...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188136</th>\n",
       "      <td>B01D1AI06Q</td>\n",
       "      <td>Good luck cutting anything with this junk. Nic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188137</th>\n",
       "      <td>B01B70WOOQ</td>\n",
       "      <td>Worked great at first.  Just before the return...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>188138 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin                                         reviewText  count\n",
       "0       B00FLYWNYQ  I was excited to try this so as soon as I got ...   7433\n",
       "1       B00COK3FD8  The tupperware was exactly as described. I lov...   4563\n",
       "2       B009HVH4XO  i have 4 of these and i didn't think they coul...   4460\n",
       "3       B00NX47YP4  Pros:\\nSeems to weigh accurately\\nEasy to use ...   4379\n",
       "4       B00902X68W  I love these sheets.  They are so soft and coz...   3725\n",
       "...            ...                                                ...    ...\n",
       "188133  B00176NOZO  I'm not sure if the one I received was defecti...      1\n",
       "188134  B00KLFM0ZW  They are really nice quality and they look rea...      1\n",
       "188135  B00FYHDHT0  Luxury look at a reasonable price.  Very elega...      1\n",
       "188136  B01D1AI06Q  Good luck cutting anything with this junk. Nic...      1\n",
       "188137  B01B70WOOQ  Worked great at first.  Just before the return...      1\n",
       "\n",
       "[188138 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print a random review text\n",
    "#df_grouped.reviewText[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_1 = df_grouped.reviewText[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2854393 characters in string vs 541632 words in a list\n"
     ]
    }
   ],
   "source": [
    "# Using the sample reviewtext\n",
    "tokens = tokenizer.tokenize(doc_1.lower())\n",
    "print('{} characters in string vs {} words in a list'.format(len(doc_1), len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535183 words in a list after removing numbers\n"
     ]
    }
   ],
   "source": [
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [token for token in tokens if not token.isnumeric()]\n",
    "print('{} words in a list after removing numbers'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291878 words in a list after words that are less than 4 chars\n"
     ]
    }
   ],
   "source": [
    "# Remove words that are less than 4 characters only\n",
    "docs = [token for token in docs if len(token) > 3]\n",
    "print('{} words in a list after words that are less than 4 chars'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234\n",
      "['hadn', 'is', 'do', 'my', 'you', 'd', 'to', 'shouldn', \"can't\", \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "#create a merged list of stop words\n",
    "nltk_stpwd = stopwords.words('english')\n",
    "#Extend stopwords with commonly found tokens in review texts\n",
    "nltk_stpwd.extend(['generally', 'used', 'personally', 'review', 'honestly','truly','whatever','done','star','one','two','three','four','five','since','ever','even','much','thing','also','go','come','must'])\n",
    "stop_words_stpwd = get_stop_words('en')\n",
    "merged_stopwords = list(set(nltk_stpwd + stop_words_stpwd))\n",
    "\n",
    "print(len(set(merged_stopwords)))\n",
    "print(merged_stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223868 words in a list after removing stop words\n"
     ]
    }
   ],
   "source": [
    "stopped_tokens = [token for token in docs if not token in merged_stopwords]\n",
    "print('{} words in a list after removing stop words'.format(len(stopped_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "##### https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_tokens = [lemmatizer.lemmatize(token) for token in stopped_tokens]\n",
    "print(lemm_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do all the preprocessing steps for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = df_grouped.shape[0]\n",
    "\n",
    "doc_set = [df_grouped.reviewText[i] for i in range(num_reviews)]\n",
    "\n",
    "texts = []\n",
    "\n",
    "for doc in doc_set:\n",
    "    # putting our 5 steps together\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    tokens_alp = [token for token in tokens if not token.isnumeric()]\n",
    "    token_gr_3 = [token for token in tokens_alp if len(token) > 3]\n",
    "    stopped_tokens = [token for token in token_gr_3 if not token in merged_stopwords]\n",
    "    lemm_tokens = [lemmatizer.lemmatize(token) for token in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(lemm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add bigrams and trigrams to docs (only ones that appear 30 times or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bigram = Phrases(texts, min_count=30)\n",
    "for idx in range(len(texts)):\n",
    "    for token in bigram[texts[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            texts[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save preprocessed pickle file for later use\n",
    "with open('texts.pkl', 'wb') as f:\n",
    "    pickle.dump(texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(292059 unique tokens: ['00pm', '02atm', '1000w', '100v', '10min']...)\n"
     ]
    }
   ],
   "source": [
    "# Gensim's Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
    "texts_dict = corpora.Dictionary(texts)\n",
    "texts_dict.save('auto_review.dict') # lets save to disk for later use\n",
    "# Examine each tokenâ€™s unique id\n",
    "print(texts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs 1 through 10: [('00pm', 0), ('02atm', 1), ('1000w', 2), ('100v', 3), ('10min', 4), ('10mins', 5), ('10minutes', 6), ('10oz', 7), ('10qt', 8), ('10th', 9)]\n"
     ]
    }
   ],
   "source": [
    "print(\"IDs 1 through 10: {}\".format(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(39976 unique tokens: ['00pm', '1000w', '10min', '10mins', '10minutes']...)\n",
      "top terms:\n",
      "[('00pm', 0), ('1000w', 1), ('10min', 2), ('10mins', 3), ('10minutes', 4), ('10oz', 5), ('10qt', 6), ('10th', 7), ('110f', 8), ('11lb', 9), ('11pm', 10), ('120v', 11), ('120vac', 12), ('12oz', 13), ('12qt', 14), ('150f', 15), ('15lbs', 16), ('15mins', 17), ('15minutes', 18), ('165f', 19), ('180f', 20), ('195f', 21), ('1960s', 22), ('1970s', 23), ('1cup', 24), ('1min', 25), ('1tbsp', 26), ('1tsp', 27), ('20ish', 28), ('20min', 29), ('20minutes', 30), ('21st', 31), ('21st_century', 32), ('220v', 33), ('22nd', 34), ('2hrs', 35), ('2lbs', 36), ('30mins', 37), ('30pm', 38), ('30sec', 39), ('3hrs', 40), ('3rds', 41), ('3times', 42), ('3yrs', 43), ('45mins', 44), ('4hrs', 45), ('4lbs', 46), ('4min', 47), ('50lb', 48), ('50th', 49), ('50th_anniversary', 50), ('5lbs', 51), ('60hz', 52), ('6lbs', 53), ('6months', 54), ('75lbs', 55), ('75th', 56), ('75th_birthday', 57), ('7lbs', 58), ('7min', 59), ('7yrs', 60), ('80lb', 61), ('8hrs', 62), ('aaaaaa', 63), ('aaah', 64), ('abandoned', 65), ('abide', 66), ('ability', 67), ('ability_adjust', 68), ('abnormally', 69), ('abou', 70), ('abound', 71), ('abrasive', 72), ('abrasive_sponge', 73), ('abruptly', 74), ('absent', 75), ('absolute', 76), ('absolute_favorite', 77), ('absolute_nightmare', 78), ('absolute_perfection', 79), ('absolutely_adore', 80), ('absolutly', 81), ('absorb', 82), ('absorb_odor', 83), ('absorbed', 84), ('absorbing', 85), ('absorbs', 86), ('absorbs_odor', 87), ('absorption', 88), ('absurd', 89), ('abundance', 90), ('abused', 91), ('abut', 92), ('abysmal', 93), ('accelerated', 94), ('accelerates', 95), ('accept', 96), ('acceptable', 97), ('accepted', 98), ('access', 99)]\n"
     ]
    }
   ],
   "source": [
    "#Filter tokens that appear too rare are too frequent\n",
    "texts_dict.filter_extremes(no_below = 20, no_above = 0.15) # inplace filter\n",
    "print(texts_dict)\n",
    "print(\"top terms:\")\n",
    "print(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188138"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create bag of words corpus\n",
    "corpus = [texts_dict.doc2bow(text) for text in texts]\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save corpus for later use\n",
    "gensim.corpora.MmCorpus.serialize('amzn_h_k_review.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load texts \n",
    "#with open('texts.pkl', 'rb') as f:\n",
    "    #texts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dictionary\n",
    "#texts_dict =  corpora.Dictionary.load('auto_review.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load corpus\n",
    "#corpus = corpora.MmCorpus('amzn_h_k_review.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build LDA model with num_topics = 7\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, alpha='auto',eta='auto',id2word=texts_dict,num_topics=7,chunksize=10000,passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model for later use\n",
    "lda_model.save('LDA_Model_grp_7_15.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To load model\n",
    "#lda_model = models.LdaModel.load('LDA_Model_grp_7_15.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get top 20 words in each topic\n",
    "top_topics = lda_model.top_topics(corpus, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.2511.\n"
     ]
    }
   ],
   "source": [
    "#Find the average topic coherence\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / 10\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "([(0.02548651, 'pillow'),\n",
      "  (0.02293437, 'sheet'),\n",
      "  (0.013917377, 'vacuum'),\n",
      "  (0.011032599, 'blanket'),\n",
      "  (0.009316477, 'fabric'),\n",
      "  (0.008612212, 'towel'),\n",
      "  (0.0072261314, 'floor'),\n",
      "  (0.0069879536, 'comforter'),\n",
      "  (0.0066475132, 'warm'),\n",
      "  (0.006570338, 'washed'),\n",
      "  (0.0054044086, 'cotton'),\n",
      "  (0.005298636, 'carpet'),\n",
      "  (0.0052566724, 'case'),\n",
      "  (0.0047472743, 'washing'),\n",
      "  (0.004519601, 'comfortable'),\n",
      "  (0.0043066745, 'blue'),\n",
      "  (0.00410424, 'throw'),\n",
      "  (0.004075046, 'bag'),\n",
      "  (0.004072287, 'king'),\n",
      "  (0.0039258823, 'hair')],\n",
      " -1.5690847396789387)\n",
      "Topic 1:\n",
      "([(0.02147665, 'knife'),\n",
      "  (0.008283081, 'cook'),\n",
      "  (0.008155044, 'oven'),\n",
      "  (0.008044792, 'cooking'),\n",
      "  (0.0073750694, 'pan'),\n",
      "  (0.007208541, 'sharp'),\n",
      "  (0.006936081, 'food'),\n",
      "  (0.005925456, 'cake'),\n",
      "  (0.0057505374, 'blade'),\n",
      "  (0.0056528873, 'bread'),\n",
      "  (0.004860285, 'heat'),\n",
      "  (0.004433428, 'tool'),\n",
      "  (0.0043840907, 'steel'),\n",
      "  (0.004277366, 'iron'),\n",
      "  (0.004179265, 'baking'),\n",
      "  (0.003952857, 'cooker'),\n",
      "  (0.003948592, 'rice'),\n",
      "  (0.0037762234, 'toaster'),\n",
      "  (0.0036455742, 'stainless'),\n",
      "  (0.0035317142, 'meat')],\n",
      " -1.6311677181135518)\n",
      "Topic 2:\n",
      "([(0.014729907, 'candle'),\n",
      "  (0.0127788, 'clock'),\n",
      "  (0.011933922, 'unit'),\n",
      "  (0.011425549, 'smell'),\n",
      "  (0.010775544, 'filter'),\n",
      "  (0.00889538, 'battery'),\n",
      "  (0.0066471277, 'scent'),\n",
      "  (0.0053706295, 'heat'),\n",
      "  (0.005225073, 'turn'),\n",
      "  (0.005029581, 'working'),\n",
      "  (0.004851348, 'setting'),\n",
      "  (0.00475298, 'hour'),\n",
      "  (0.004419745, 'timer'),\n",
      "  (0.0042479057, 'power'),\n",
      "  (0.004236291, 'button'),\n",
      "  (0.0042037116, 'night'),\n",
      "  (0.0040459395, 'quiet'),\n",
      "  (0.0037455377, 'scale'),\n",
      "  (0.0037129691, 'burn'),\n",
      "  (0.00360289, 'loud')],\n",
      " -1.647728050957479)\n",
      "Topic 3:\n",
      "([(0.024317482, 'coffee'),\n",
      "  (0.011863715, 'bowl'),\n",
      "  (0.008937708, 'container'),\n",
      "  (0.007415723, 'drink'),\n",
      "  (0.007389918, 'cup'),\n",
      "  (0.0066962554, 'food'),\n",
      "  (0.006341557, 'machine'),\n",
      "  (0.005306786, 'cold'),\n",
      "  (0.005299774, 'lunch'),\n",
      "  (0.0051748664, 'bag'),\n",
      "  (0.004978749, 'maker'),\n",
      "  (0.0049354765, 'dishwasher'),\n",
      "  (0.004796019, 'bottle'),\n",
      "  (0.004597787, 'seal'),\n",
      "  (0.0045310995, 'dish'),\n",
      "  (0.0044037, 'lid'),\n",
      "  (0.003978351, 'leak'),\n",
      "  (0.003800174, 'plate'),\n",
      "  (0.003552725, 'mug'),\n",
      "  (0.0035265821, 'microwave')],\n",
      " -1.6561096237710984)\n",
      "Topic 4:\n",
      "([(0.007413203, 'frame'),\n",
      "  (0.0067321043, 'wall'),\n",
      "  (0.0062294602, 'shelf'),\n",
      "  (0.005667018, 'tree'),\n",
      "  (0.005458777, 'wood'),\n",
      "  (0.00486133, 'party'),\n",
      "  (0.004457704, 'hang'),\n",
      "  (0.004378446, 'stand'),\n",
      "  (0.004019319, 'ornament'),\n",
      "  (0.0039932304, 'door'),\n",
      "  (0.0038866254, 'drawer'),\n",
      "  (0.003707056, 'rack'),\n",
      "  (0.003676945, 'adorable'),\n",
      "  (0.00357861, 'storage'),\n",
      "  (0.0035745192, 'screw'),\n",
      "  (0.0033896747, 'hanger'),\n",
      "  (0.003342472, 'broken'),\n",
      "  (0.0030864375, 'assemble'),\n",
      "  (0.0029463037, 'holder'),\n",
      "  (0.0029053264, 'hanging')],\n",
      " -1.8069996455487336)\n",
      "Topic 5:\n",
      "([(0.02575997, 'chair'),\n",
      "  (0.019111529, 'mattress'),\n",
      "  (0.015160591, 'comfortable'),\n",
      "  (0.014105529, 'wine'),\n",
      "  (0.009871103, 'bottle'),\n",
      "  (0.009201988, 'pillow'),\n",
      "  (0.0075199273, 'foam'),\n",
      "  (0.006562449, 'towel'),\n",
      "  (0.006043532, 'sleep'),\n",
      "  (0.0058488883, 'seat'),\n",
      "  (0.00575641, 'opener'),\n",
      "  (0.005623039, 'support'),\n",
      "  (0.0052033593, 'desk'),\n",
      "  (0.0050000837, 'neck'),\n",
      "  (0.0049032182, 'firm'),\n",
      "  (0.0047968966, 'night'),\n",
      "  (0.0046051545, 'stool'),\n",
      "  (0.0042675114, 'leg'),\n",
      "  (0.0039252513, 'hour'),\n",
      "  (0.0038893018, 'memory')],\n",
      " -2.068286475546528)\n",
      "Topic 6:\n",
      "([(0.025875453, 'curtain'),\n",
      "  (0.019432394, 'shower'),\n",
      "  (0.0149910655, 'bathroom'),\n",
      "  (0.010593263, 'board'),\n",
      "  (0.009125681, 'window'),\n",
      "  (0.0090893665, 'spoon'),\n",
      "  (0.008999729, 'soap'),\n",
      "  (0.008825494, 'mold'),\n",
      "  (0.0076722335, 'tray'),\n",
      "  (0.007136124, 'shower_curtain'),\n",
      "  (0.0070861476, 'sink'),\n",
      "  (0.006615621, 'salt'),\n",
      "  (0.005518872, 'bottle'),\n",
      "  (0.00549613, 'dish'),\n",
      "  (0.0054664565, 'pepper'),\n",
      "  (0.0050691958, 'liner'),\n",
      "  (0.004820566, 'counter'),\n",
      "  (0.004696216, 'holder'),\n",
      "  (0.004682157, 'spice'),\n",
      "  (0.0046150354, 'bath')],\n",
      " -2.1315717149502866)\n"
     ]
    }
   ],
   "source": [
    "#Print topics and word distribution for each topic\n",
    "counter = 0\n",
    "for topic in top_topics:\n",
    "    print('Topic {}:'.format(counter))\n",
    "    counter += 1\n",
    "    pprint(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "#Prepare topic visualization\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, texts_dict,sort_topics = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pyLDAvis as html\n",
    "pyLDAvis.save_html(vis,\"vis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 6, 5, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "print(vis.topic_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The topic order here is different than gensim\n",
    "#### Notice that in gensim topic starte from 0, in PyLDAvis topic starts from 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
