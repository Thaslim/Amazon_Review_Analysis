{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling By Grouping Multiple Similar Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19525627, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the preprocessed data file\n",
    "df_huge = pd.read_pickle('../Clean_data')\n",
    "df_huge.head()\n",
    "df_huge.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580956</th>\n",
       "      <td>B00FLYWNYQ</td>\n",
       "      <td>This is my first pressure cooker so I wasn't r...</td>\n",
       "      <td>22704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836347</th>\n",
       "      <td>B00Q7EV29G</td>\n",
       "      <td>This mattress shipped free via 2 day Prime Shi...</td>\n",
       "      <td>15201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551006</th>\n",
       "      <td>B00EINBSJ2</td>\n",
       "      <td>I received my pillow on time as stated on Frid...</td>\n",
       "      <td>14856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409335</th>\n",
       "      <td>B009HVH4XO</td>\n",
       "      <td>i have 4 of these and i didn't think they coul...</td>\n",
       "      <td>14649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742237</th>\n",
       "      <td>B00LV4W8BI</td>\n",
       "      <td>The sheets are very soft and comfortable right...</td>\n",
       "      <td>13507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790907</th>\n",
       "      <td>B00NX47YP4</td>\n",
       "      <td>I love this little scale. It's simple, easy to...</td>\n",
       "      <td>12971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102682</th>\n",
       "      <td>B019D9HESO</td>\n",
       "      <td>LOVE this cup! Keeps my drink from getting wat...</td>\n",
       "      <td>11770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420105</th>\n",
       "      <td>B009ZJ2M7G</td>\n",
       "      <td>My unreliable Bissell 2x scrubber recently bro...</td>\n",
       "      <td>10518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391650</th>\n",
       "      <td>B00902X68W</td>\n",
       "      <td>The sheets are some of the softest and  most c...</td>\n",
       "      <td>10026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503326</th>\n",
       "      <td>B00COK3FD8</td>\n",
       "      <td>The tupperware was exactly as described. I lov...</td>\n",
       "      <td>9171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393824</th>\n",
       "      <td>B0091YYUAM</td>\n",
       "      <td>Mopping is one of those things I really dislik...</td>\n",
       "      <td>8896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366193</th>\n",
       "      <td>B007WQ9YNE</td>\n",
       "      <td>I used to think that a rice cooker was a pot o...</td>\n",
       "      <td>8872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908309</th>\n",
       "      <td>B00VANO9OE</td>\n",
       "      <td>These are the softest sheets that I've ever fe...</td>\n",
       "      <td>8297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86088</th>\n",
       "      <td>B001892AX2</td>\n",
       "      <td>Mine arrived yesterday; timely, in good shape,...</td>\n",
       "      <td>8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662479</th>\n",
       "      <td>B00IR77HOK</td>\n",
       "      <td>I got a 16 oz one. The lid is very well design...</td>\n",
       "      <td>7987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934554</th>\n",
       "      <td>B00X8KSKF6</td>\n",
       "      <td>I love these bags, I was given the opportunity...</td>\n",
       "      <td>7945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532317</th>\n",
       "      <td>B00DU1R19E</td>\n",
       "      <td>The quality is good for the low price. It seem...</td>\n",
       "      <td>7837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782825</th>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets came on time, quality is superb, ...</td>\n",
       "      <td>7690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122028</th>\n",
       "      <td>B001PB8EJ2</td>\n",
       "      <td>Quick and good.  This is my second one.  The b...</td>\n",
       "      <td>7543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754899</th>\n",
       "      <td>B00MG2OOHK</td>\n",
       "      <td>Easy to use, fast results, very good price. I ...</td>\n",
       "      <td>7311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018251</th>\n",
       "      <td>B014P3B7TU</td>\n",
       "      <td>I just received this vacuum today. My stairs h...</td>\n",
       "      <td>7306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692139</th>\n",
       "      <td>B00JVLG2KG</td>\n",
       "      <td>This thermos is the BEST!  I have opened it an...</td>\n",
       "      <td>7227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>B00009R66F</td>\n",
       "      <td>Purchased this product after looking around an...</td>\n",
       "      <td>7198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55148</th>\n",
       "      <td>B000RL1WNQ</td>\n",
       "      <td>It is definitely tough shopping by looking onl...</td>\n",
       "      <td>7186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095104</th>\n",
       "      <td>B018UQ5AMS</td>\n",
       "      <td>It's awesome nice hot cup every time //Outstan...</td>\n",
       "      <td>7183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61595</th>\n",
       "      <td>B000VENLF6</td>\n",
       "      <td>Very happy with the Pike Street overfilled ult...</td>\n",
       "      <td>7175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67600</th>\n",
       "      <td>B000YGEVMI</td>\n",
       "      <td>My 3 year old son had this thermos and loved i...</td>\n",
       "      <td>7149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978800</th>\n",
       "      <td>B0118DYK3K</td>\n",
       "      <td>Awesome product. My husband and I bought 2 of ...</td>\n",
       "      <td>7036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>B00005UP2N</td>\n",
       "      <td>We've had our Kitchenaid for about 15 years no...</td>\n",
       "      <td>6867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558510</th>\n",
       "      <td>B00ETP7D3E</td>\n",
       "      <td>This was completely worth the money!  Keeps co...</td>\n",
       "      <td>6813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               asin                                         reviewText  count\n",
       "580956   B00FLYWNYQ  This is my first pressure cooker so I wasn't r...  22704\n",
       "836347   B00Q7EV29G  This mattress shipped free via 2 day Prime Shi...  15201\n",
       "551006   B00EINBSJ2  I received my pillow on time as stated on Frid...  14856\n",
       "409335   B009HVH4XO  i have 4 of these and i didn't think they coul...  14649\n",
       "742237   B00LV4W8BI  The sheets are very soft and comfortable right...  13507\n",
       "790907   B00NX47YP4  I love this little scale. It's simple, easy to...  12971\n",
       "1102682  B019D9HESO  LOVE this cup! Keeps my drink from getting wat...  11770\n",
       "420105   B009ZJ2M7G  My unreliable Bissell 2x scrubber recently bro...  10518\n",
       "391650   B00902X68W  The sheets are some of the softest and  most c...  10026\n",
       "503326   B00COK3FD8  The tupperware was exactly as described. I lov...   9171\n",
       "393824   B0091YYUAM  Mopping is one of those things I really dislik...   8896\n",
       "366193   B007WQ9YNE  I used to think that a rice cooker was a pot o...   8872\n",
       "908309   B00VANO9OE  These are the softest sheets that I've ever fe...   8297\n",
       "86088    B001892AX2  Mine arrived yesterday; timely, in good shape,...   8154\n",
       "662479   B00IR77HOK  I got a 16 oz one. The lid is very well design...   7987\n",
       "934554   B00X8KSKF6  I love these bags, I was given the opportunity...   7945\n",
       "532317   B00DU1R19E  The quality is good for the low price. It seem...   7837\n",
       "782825   B00NLLUNSE  These sheets came on time, quality is superb, ...   7690\n",
       "122028   B001PB8EJ2  Quick and good.  This is my second one.  The b...   7543\n",
       "754899   B00MG2OOHK  Easy to use, fast results, very good price. I ...   7311\n",
       "1018251  B014P3B7TU  I just received this vacuum today. My stairs h...   7306\n",
       "692139   B00JVLG2KG  This thermos is the BEST!  I have opened it an...   7227\n",
       "5393     B00009R66F  Purchased this product after looking around an...   7198\n",
       "55148    B000RL1WNQ  It is definitely tough shopping by looking onl...   7186\n",
       "1095104  B018UQ5AMS  It's awesome nice hot cup every time //Outstan...   7183\n",
       "61595    B000VENLF6  Very happy with the Pike Street overfilled ult...   7175\n",
       "67600    B000YGEVMI  My 3 year old son had this thermos and loved i...   7149\n",
       "978800   B0118DYK3K  Awesome product. My husband and I bought 2 of ...   7036\n",
       "3173     B00005UP2N  We've had our Kitchenaid for about 15 years no...   6867\n",
       "558510   B00ETP7D3E  This was completely worth the money!  Keeps co...   6813"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selection of product- product with maximum review data\n",
    "df_grouped = df_huge.groupby(['asin']).agg({'reviewText': ' ** '.join,'asin':'size'}).rename(columns={'asin':'count'}).reset_index()\n",
    "df_grouped.sort_values(by = 'count',ascending=False,inplace=True)\n",
    "df_grouped.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580956</th>\n",
       "      <td>B00FLYWNYQ</td>\n",
       "      <td>This is my first pressure cooker so I wasn't r...</td>\n",
       "      <td>22704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836347</th>\n",
       "      <td>B00Q7EV29G</td>\n",
       "      <td>This mattress shipped free via 2 day Prime Shi...</td>\n",
       "      <td>15201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551006</th>\n",
       "      <td>B00EINBSJ2</td>\n",
       "      <td>I received my pillow on time as stated on Frid...</td>\n",
       "      <td>14856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409335</th>\n",
       "      <td>B009HVH4XO</td>\n",
       "      <td>i have 4 of these and i didn't think they coul...</td>\n",
       "      <td>14649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742237</th>\n",
       "      <td>B00LV4W8BI</td>\n",
       "      <td>The sheets are very soft and comfortable right...</td>\n",
       "      <td>13507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790907</th>\n",
       "      <td>B00NX47YP4</td>\n",
       "      <td>I love this little scale. It's simple, easy to...</td>\n",
       "      <td>12971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102682</th>\n",
       "      <td>B019D9HESO</td>\n",
       "      <td>LOVE this cup! Keeps my drink from getting wat...</td>\n",
       "      <td>11770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420105</th>\n",
       "      <td>B009ZJ2M7G</td>\n",
       "      <td>My unreliable Bissell 2x scrubber recently bro...</td>\n",
       "      <td>10518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391650</th>\n",
       "      <td>B00902X68W</td>\n",
       "      <td>The sheets are some of the softest and  most c...</td>\n",
       "      <td>10026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503326</th>\n",
       "      <td>B00COK3FD8</td>\n",
       "      <td>The tupperware was exactly as described. I lov...</td>\n",
       "      <td>9171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393824</th>\n",
       "      <td>B0091YYUAM</td>\n",
       "      <td>Mopping is one of those things I really dislik...</td>\n",
       "      <td>8896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366193</th>\n",
       "      <td>B007WQ9YNE</td>\n",
       "      <td>I used to think that a rice cooker was a pot o...</td>\n",
       "      <td>8872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908309</th>\n",
       "      <td>B00VANO9OE</td>\n",
       "      <td>These are the softest sheets that I've ever fe...</td>\n",
       "      <td>8297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86088</th>\n",
       "      <td>B001892AX2</td>\n",
       "      <td>Mine arrived yesterday; timely, in good shape,...</td>\n",
       "      <td>8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662479</th>\n",
       "      <td>B00IR77HOK</td>\n",
       "      <td>I got a 16 oz one. The lid is very well design...</td>\n",
       "      <td>7987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934554</th>\n",
       "      <td>B00X8KSKF6</td>\n",
       "      <td>I love these bags, I was given the opportunity...</td>\n",
       "      <td>7945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532317</th>\n",
       "      <td>B00DU1R19E</td>\n",
       "      <td>The quality is good for the low price. It seem...</td>\n",
       "      <td>7837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782825</th>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets came on time, quality is superb, ...</td>\n",
       "      <td>7690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122028</th>\n",
       "      <td>B001PB8EJ2</td>\n",
       "      <td>Quick and good.  This is my second one.  The b...</td>\n",
       "      <td>7543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754899</th>\n",
       "      <td>B00MG2OOHK</td>\n",
       "      <td>Easy to use, fast results, very good price. I ...</td>\n",
       "      <td>7311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018251</th>\n",
       "      <td>B014P3B7TU</td>\n",
       "      <td>I just received this vacuum today. My stairs h...</td>\n",
       "      <td>7306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692139</th>\n",
       "      <td>B00JVLG2KG</td>\n",
       "      <td>This thermos is the BEST!  I have opened it an...</td>\n",
       "      <td>7227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5393</th>\n",
       "      <td>B00009R66F</td>\n",
       "      <td>Purchased this product after looking around an...</td>\n",
       "      <td>7198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55148</th>\n",
       "      <td>B000RL1WNQ</td>\n",
       "      <td>It is definitely tough shopping by looking onl...</td>\n",
       "      <td>7186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095104</th>\n",
       "      <td>B018UQ5AMS</td>\n",
       "      <td>It's awesome nice hot cup every time //Outstan...</td>\n",
       "      <td>7183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61595</th>\n",
       "      <td>B000VENLF6</td>\n",
       "      <td>Very happy with the Pike Street overfilled ult...</td>\n",
       "      <td>7175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67600</th>\n",
       "      <td>B000YGEVMI</td>\n",
       "      <td>My 3 year old son had this thermos and loved i...</td>\n",
       "      <td>7149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978800</th>\n",
       "      <td>B0118DYK3K</td>\n",
       "      <td>Awesome product. My husband and I bought 2 of ...</td>\n",
       "      <td>7036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>B00005UP2N</td>\n",
       "      <td>We've had our Kitchenaid for about 15 years no...</td>\n",
       "      <td>6867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558510</th>\n",
       "      <td>B00ETP7D3E</td>\n",
       "      <td>This was completely worth the money!  Keeps co...</td>\n",
       "      <td>6813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660205</th>\n",
       "      <td>B00IOEFBKS</td>\n",
       "      <td>This review is for the BDH2000PL MAX Lithium P...</td>\n",
       "      <td>6776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933376</th>\n",
       "      <td>B00X5ETKU4</td>\n",
       "      <td>Everything works as advertised.  Makes a delic...</td>\n",
       "      <td>6750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977379</th>\n",
       "      <td>B0113UZJE2</td>\n",
       "      <td>I have been using a digital postal scale that ...</td>\n",
       "      <td>6734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313498</th>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>6395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157926</th>\n",
       "      <td>B002S52ZDU</td>\n",
       "      <td>They seem like they will hold up well.  They a...</td>\n",
       "      <td>6302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215190</th>\n",
       "      <td>B01G7SBOF8</td>\n",
       "      <td>Learn how to make good popcorn.  What can you ...</td>\n",
       "      <td>6273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215191</th>\n",
       "      <td>B01G7SGOL2</td>\n",
       "      <td>This is a happy little popcorn popper that mak...</td>\n",
       "      <td>6269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969161</th>\n",
       "      <td>B010B0CA9W</td>\n",
       "      <td>Excellent quality!!! Beautiful color!!! //grea...</td>\n",
       "      <td>6259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154684</th>\n",
       "      <td>B01CMT8DFU</td>\n",
       "      <td>&lt;div id=\"video-block-R1S57HZN03N4PZ\" class=\"a-...</td>\n",
       "      <td>6233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46118</th>\n",
       "      <td>B000N4WN08</td>\n",
       "      <td>I purchased this pot becuase I needed a large ...</td>\n",
       "      <td>6159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454616</th>\n",
       "      <td>B00B5WQSL2</td>\n",
       "      <td>I received this item very quickly, good condit...</td>\n",
       "      <td>6104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23488</th>\n",
       "      <td>B000CO89T8</td>\n",
       "      <td>Ordered this item believing it was unit that h...</td>\n",
       "      <td>5976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520770</th>\n",
       "      <td>B00DDXWFY0</td>\n",
       "      <td>wasn't sure how it was going to work. cooked e...</td>\n",
       "      <td>5947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876619</th>\n",
       "      <td>B00T6TKYPC</td>\n",
       "      <td>Nice quality trash can. Great silent closing l...</td>\n",
       "      <td>5906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619394</th>\n",
       "      <td>B00H1QWD6A</td>\n",
       "      <td>As my husband said \"this was ridiculously easy...</td>\n",
       "      <td>5890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556553</th>\n",
       "      <td>B00EQT70QS</td>\n",
       "      <td>Mixing - The two speed operation is handy.  Fo...</td>\n",
       "      <td>5790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442156</th>\n",
       "      <td>B00ARQVLGO</td>\n",
       "      <td>Mixing - The two speed operation is handy.  Fo...</td>\n",
       "      <td>5777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407962</th>\n",
       "      <td>B009GCXETW</td>\n",
       "      <td>This drawer fits perfect under my Keurig, whic...</td>\n",
       "      <td>5760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447585</th>\n",
       "      <td>B00AYULZEQ</td>\n",
       "      <td>It's very light but more than sturdy enough fo...</td>\n",
       "      <td>5737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>B000BWZ7QO</td>\n",
       "      <td>These are really heavy duty mason jars.  I use...</td>\n",
       "      <td>5736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               asin                                         reviewText  count\n",
       "580956   B00FLYWNYQ  This is my first pressure cooker so I wasn't r...  22704\n",
       "836347   B00Q7EV29G  This mattress shipped free via 2 day Prime Shi...  15201\n",
       "551006   B00EINBSJ2  I received my pillow on time as stated on Frid...  14856\n",
       "409335   B009HVH4XO  i have 4 of these and i didn't think they coul...  14649\n",
       "742237   B00LV4W8BI  The sheets are very soft and comfortable right...  13507\n",
       "790907   B00NX47YP4  I love this little scale. It's simple, easy to...  12971\n",
       "1102682  B019D9HESO  LOVE this cup! Keeps my drink from getting wat...  11770\n",
       "420105   B009ZJ2M7G  My unreliable Bissell 2x scrubber recently bro...  10518\n",
       "391650   B00902X68W  The sheets are some of the softest and  most c...  10026\n",
       "503326   B00COK3FD8  The tupperware was exactly as described. I lov...   9171\n",
       "393824   B0091YYUAM  Mopping is one of those things I really dislik...   8896\n",
       "366193   B007WQ9YNE  I used to think that a rice cooker was a pot o...   8872\n",
       "908309   B00VANO9OE  These are the softest sheets that I've ever fe...   8297\n",
       "86088    B001892AX2  Mine arrived yesterday; timely, in good shape,...   8154\n",
       "662479   B00IR77HOK  I got a 16 oz one. The lid is very well design...   7987\n",
       "934554   B00X8KSKF6  I love these bags, I was given the opportunity...   7945\n",
       "532317   B00DU1R19E  The quality is good for the low price. It seem...   7837\n",
       "782825   B00NLLUNSE  These sheets came on time, quality is superb, ...   7690\n",
       "122028   B001PB8EJ2  Quick and good.  This is my second one.  The b...   7543\n",
       "754899   B00MG2OOHK  Easy to use, fast results, very good price. I ...   7311\n",
       "1018251  B014P3B7TU  I just received this vacuum today. My stairs h...   7306\n",
       "692139   B00JVLG2KG  This thermos is the BEST!  I have opened it an...   7227\n",
       "5393     B00009R66F  Purchased this product after looking around an...   7198\n",
       "55148    B000RL1WNQ  It is definitely tough shopping by looking onl...   7186\n",
       "1095104  B018UQ5AMS  It's awesome nice hot cup every time //Outstan...   7183\n",
       "61595    B000VENLF6  Very happy with the Pike Street overfilled ult...   7175\n",
       "67600    B000YGEVMI  My 3 year old son had this thermos and loved i...   7149\n",
       "978800   B0118DYK3K  Awesome product. My husband and I bought 2 of ...   7036\n",
       "3173     B00005UP2N  We've had our Kitchenaid for about 15 years no...   6867\n",
       "558510   B00ETP7D3E  This was completely worth the money!  Keeps co...   6813\n",
       "660205   B00IOEFBKS  This review is for the BDH2000PL MAX Lithium P...   6776\n",
       "933376   B00X5ETKU4  Everything works as advertised.  Makes a delic...   6750\n",
       "977379   B0113UZJE2  I have been using a digital postal scale that ...   6734\n",
       "313498   B00635VODS  This is a fine comforter, super soft and fluff...   6395\n",
       "157926   B002S52ZDU  They seem like they will hold up well.  They a...   6302\n",
       "1215190  B01G7SBOF8  Learn how to make good popcorn.  What can you ...   6273\n",
       "1215191  B01G7SGOL2  This is a happy little popcorn popper that mak...   6269\n",
       "969161   B010B0CA9W  Excellent quality!!! Beautiful color!!! //grea...   6259\n",
       "1154684  B01CMT8DFU  <div id=\"video-block-R1S57HZN03N4PZ\" class=\"a-...   6233\n",
       "46118    B000N4WN08  I purchased this pot becuase I needed a large ...   6159\n",
       "454616   B00B5WQSL2  I received this item very quickly, good condit...   6104\n",
       "23488    B000CO89T8  Ordered this item believing it was unit that h...   5976\n",
       "520770   B00DDXWFY0  wasn't sure how it was going to work. cooked e...   5947\n",
       "876619   B00T6TKYPC  Nice quality trash can. Great silent closing l...   5906\n",
       "619394   B00H1QWD6A  As my husband said \"this was ridiculously easy...   5890\n",
       "556553   B00EQT70QS  Mixing - The two speed operation is handy.  Fo...   5790\n",
       "442156   B00ARQVLGO  Mixing - The two speed operation is handy.  Fo...   5777\n",
       "407962   B009GCXETW  This drawer fits perfect under my Keurig, whic...   5760\n",
       "447585   B00AYULZEQ  It's very light but more than sturdy enough fo...   5737\n",
       "22502    B000BWZ7QO  These are really heavy duty mason jars.  I use...   5736"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Sheets =  df_huge.loc[df_huge['asin'].isin([\"B00LV4W8BI\",'B00902X68W',\"B00NLLUNSE\", \"B00635VODS\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37618, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Sheets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    37618.000000\n",
       "mean         4.277872\n",
       "std          1.245247\n",
       "min          1.000000\n",
       "25%          4.000000\n",
       "50%          5.000000\n",
       "75%          5.000000\n",
       "max          5.000000\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Sheets['overall'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    25456\n",
       "4.0     4715\n",
       "1.0     2832\n",
       "3.0     2723\n",
       "2.0     1892\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Sheets['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the dataframe with review sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>\\nI especially like the price was not high but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>I am comparing it to both a 'down' and anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>\"Down\" is difficult to clean and expensive as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>We'll see how it holds up in the wash and dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134073</th>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134074</th>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>They are not too heavy, which is a sign that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134075</th>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>At this price point it was easy to order seve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134076</th>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>I ordered the additional pillowcases for each...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134077</th>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>//Wonderful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134078 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           reviewerID                                          sentences\n",
       "0       AQDRWVTBXGNIZ  This is a fine comforter, super soft and fluff...\n",
       "1       AQDRWVTBXGNIZ  \\nI especially like the price was not high but...\n",
       "2       AQDRWVTBXGNIZ   I am comparing it to both a 'down' and anothe...\n",
       "3       AQDRWVTBXGNIZ   \"Down\" is difficult to clean and expensive as...\n",
       "4       AQDRWVTBXGNIZ      We'll see how it holds up in the wash and dry\n",
       "...               ...                                                ...\n",
       "134073  AGLA12LDPFWU4  These sheets are light and comfortable! They f...\n",
       "134074  AGLA12LDPFWU4   They are not too heavy, which is a sign that ...\n",
       "134075  AGLA12LDPFWU4   At this price point it was easy to order seve...\n",
       "134076  AGLA12LDPFWU4   I ordered the additional pillowcases for each...\n",
       "134077  AGLA12LDPFWU4                                        //Wonderful\n",
       "\n",
       "[134078 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preapre a data frame by spliiting reviews into sentences\n",
    "df_sentences = pd.DataFrame(df_Sheets.reviewText.str.split('.').tolist(), index=df_Sheets.reviewerID).stack()\n",
    "df_sentences = df_sentences.reset_index([0, 'reviewerID'])\n",
    "df_sentences.columns = ['reviewerID', 'sentences']\n",
    "\n",
    "df_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>rev_date</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>\\nI especially like the price was not high but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>I am comparing it to both a 'down' and anothe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>\"Down\" is difficult to clean and expensive as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>We'll see how it holds up in the wash and dry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136563</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136564</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>They are not too heavy, which is a sign that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136565</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>At this price point it was easy to order seve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136566</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>I ordered the additional pillowcases for each...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136567</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>//Wonderful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136568 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall     reviewerID        asin  \\\n",
       "0           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "1           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "2           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "3           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "4           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "...         ...            ...         ...   \n",
       "136563      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136564      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136565      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136566      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136567      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "\n",
       "                                               reviewText   rev_date  \\\n",
       "0       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "1       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "2       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "3       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "4       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "...                                                   ...        ...   \n",
       "136563  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136564  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136565  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136566  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136567  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "\n",
       "                                                sentences  \n",
       "0       This is a fine comforter, super soft and fluff...  \n",
       "1       \\nI especially like the price was not high but...  \n",
       "2        I am comparing it to both a 'down' and anothe...  \n",
       "3        \"Down\" is difficult to clean and expensive as...  \n",
       "4           We'll see how it holds up in the wash and dry  \n",
       "...                                                   ...  \n",
       "136563  These sheets are light and comfortable! They f...  \n",
       "136564   They are not too heavy, which is a sign that ...  \n",
       "136565   At this price point it was easy to order seve...  \n",
       "136566   I ordered the additional pillowcases for each...  \n",
       "136567                                        //Wonderful  \n",
       "\n",
       "[136568 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging the 50 reviews dataframe having all columns with new sentences dataframe\n",
    "df_full = df_Sheets.merge(df_sentences, how='inner', left_on='reviewerID', right_on='reviewerID')\n",
    "df_full\n",
    "\n",
    "# #  Removed unneccesary columns \n",
    "# df_50p_5800r = df_50p_5800r.drop([\"reviewText\", \"rev_date\", \"product_ID\"], axis=1)\n",
    "# df_50p_5800r = df_50p_5800r\n",
    "\n",
    "# df_B00ETP7D3E_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Otherwise, they're soft and the tan color is matches the photo\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.iloc[134079]['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a fine comforter, super soft and fluffy but not too hot at night'] 134078\n"
     ]
    }
   ],
   "source": [
    "# Get sentences from the data frame\n",
    "list_sentences = df_sentences.sentences.values.tolist()\n",
    "print(list_sentences[:1], len(list_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is', 'fine', 'comforter', 'super', 'soft', 'and', 'fluffy', 'but', 'not', 'too', 'hot', 'at', 'night']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "list_sentences_tokenized = list(sent_to_words(list_sentences))\n",
    "list_sentences_tokenized_copy = list(sent_to_words(list_sentences))\n",
    "print(list_sentences_tokenized_copy[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'fine', 'comforter', 'super', 'soft', 'and', 'fluffy', 'but', 'not', 'too', 'hot', 'at', 'night']\n",
      "--- 12.095656156539917 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "bigram = gensim.models.Phrases(list_sentences_tokenized, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[list_sentences_tokenized], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[list_sentences_tokenized[0]]])\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fine', 'comforter', 'super', 'soft', 'fluffy', 'hot', 'night']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(list_sentences_tokenized)\n",
    "data_words_nostops[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fine', 'comforter', 'super', 'soft', 'fluffy', 'hot', 'night']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "data_words_bigrams[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fine', 'comforter', 'super', 'soft', 'fluffy', 'hot', 'night']]\n",
      "--- 224.05514192581177 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For Text Blob\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "list_sentences_lemetised = []\n",
    "for i in range(len(data_lemmatized)):\n",
    "    lem_sen = TreebankWordDetokenizer().detokenize(data_lemmatized[i])\n",
    "    list_sentences_lemetised.append(lem_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fine comforter super soft fluffy hot night',\n",
       " 'especially price high let low price fool nicely make high quality',\n",
       " 'compare alternative comforter also',\n",
       " 'difficult expensive clean warm cozy light',\n",
       " 'see hold dry',\n",
       " 'also arrive quickly',\n",
       " 'good',\n",
       " 'order',\n",
       " 'show promptly however king',\n",
       " 'ship seem order',\n",
       " '',\n",
       " 'expect thick fluffy comforter',\n",
       " '',\n",
       " 'nervous',\n",
       " '',\n",
       " 'good price would cost ship back decide give try',\n",
       " '',\n",
       " 'turn love fact super thick way fit washer feel smothered blanket',\n",
       " '',\n",
       " 'still manage give extraordinary warmth leg get warm run hot',\n",
       " '',\n",
       " 'perfect hubby happier get great price boot',\n",
       " 'know want comforter even know love seriously wait snuggle bed soft buy sheet set go feel millionaire eye finally flutter comfort',\n",
       " '',\n",
       " '',\n",
       " 'soft warm',\n",
       " 'nice high quality comforter',\n",
       " 'really soft appear well make',\n",
       " 'would actually recomme would keep truely king',\n",
       " 'regular low profile king sized bed barely make box_spring past mattress',\n",
       " 'would perfect sized bed',\n",
       " 'wanted would hang close picture show',\n",
       " '',\n",
       " 'overly wonderful',\n",
       " '',\n",
       " 'make feel think alternative',\n",
       " '',\n",
       " 'also barely cover king sized bed look small',\n",
       " '',\n",
       " 'certainly buy',\n",
       " '',\n",
       " 'also keep mind take cycle super capacity year old dryer order dry completely efficient',\n",
       " 'thin light weight warm',\n",
       " 'feel light comforter',\n",
       " 'warm enough flannel sheet blanket',\n",
       " 'warm',\n",
       " 'warm enough',\n",
       " 'confortable look great stiched blocke bunch must deal awesome',\n",
       " 'love soft',\n",
       " 'also quite roll move around',\n",
       " 'great quality great price would buy soft quiet',\n",
       " 'excellent duvet people allergic feather',\n",
       " 'comforter replace allergy',\n",
       " 'comfortable well make',\n",
       " 'purchase crate barrel year ago far expensive believe much well deal',\n",
       " 'highly recommend product',\n",
       " 'amazing quality',\n",
       " 'nice light weight also soft',\n",
       " 'stitching seem good quality day speak longevity',\n",
       " 'nice',\n",
       " '',\n",
       " '',\n",
       " 'nice',\n",
       " '',\n",
       " 'heavy',\n",
       " 'look wonderful duvet cover pill stitching keep bunch',\n",
       " 'get pay',\n",
       " 'beautiful',\n",
       " 'purchase insert give well fit new bedspread buy immediately wash',\n",
       " 'wash well dry low set star',\n",
       " 'make comfortable',\n",
       " 'make comfortable',\n",
       " '',\n",
       " 'love love love',\n",
       " 'comfortable soft',\n",
       " 'problem keep grandson faceplant bed',\n",
       " 'love',\n",
       " 'want family keep butts bed',\n",
       " 'bed fit perfectly',\n",
       " '',\n",
       " '',\n",
       " 'mention comfortable blanket worth money',\n",
       " 'love blanket',\n",
       " 'recently purchase comforter proudly say happy',\n",
       " 'comforter make sturdy material feel somewhat soft reliable touch',\n",
       " 'fabric distribution feather individual cell comforter opinion seem perfect',\n",
       " 'see many comforter struggle ratio feather problem',\n",
       " 'pull box first fear seem thin warm enough basement bedroom temperature sometimes drop degree',\n",
       " 'night comforter prove wrong',\n",
       " 'would strongly recommend comforter look buy new',\n",
       " 'really',\n",
       " '',\n",
       " 'insert',\n",
       " 'skeptical give review thing pretty good',\n",
       " 'look little thin arrived mail',\n",
       " 'however give shoot super warm min climb bed think really',\n",
       " '',\n",
       " '',\n",
       " 'take long switch duvet insert complaint regard size',\n",
       " '',\n",
       " 'complaint quality know pay close attention dimension time make purchase duvete huh',\n",
       " 'pleased product',\n",
       " 'night shed pille',\n",
       " 'wash sure hold sew square give hope hold pretty well',\n",
       " 'nice comforter insert think good buy',\n",
       " 'live degree day degree night think good product keep toasty need quite much heat sleep',\n",
       " '',\n",
       " 'really warm great quality especially price',\n",
       " 'put cover look feel real goose',\n",
       " 'great product',\n",
       " 'buy anyways',\n",
       " 'soft decent thickness thin thick',\n",
       " 'perfect look extra warmth fullness',\n",
       " 'go try bed notice tiny bit greenish lint piling',\n",
       " 'figured material must kind lint cling',\n",
       " 'soft notice lint piling',\n",
       " 'duvet perfect weight',\n",
       " 'thick thin',\n",
       " 'perfect',\n",
       " 'purchase',\n",
       " 'nice lightweight',\n",
       " 'get month ago hot right perfect size bed look beautiful feel real duvet insert',\n",
       " '',\n",
       " 'allergic goose order',\n",
       " 'can beat price',\n",
       " 'go look really nice comfortable',\n",
       " 'great price great comforter',\n",
       " 'price comforter perfect',\n",
       " 'look wonderful fit perfectly',\n",
       " 'wash yet believe fine',\n",
       " 'perfect comforter look great',\n",
       " 'love warm heavy',\n",
       " 'nice weight',\n",
       " 'right size',\n",
       " 'husband always cold even throw time',\n",
       " '',\n",
       " 'would definitely recommend',\n",
       " 'love',\n",
       " 'really high hope comforter make decision buy alternative comforter oppose real thought would better',\n",
       " 'really compare real pretty warm',\n",
       " 'also relatively thin flat however disappointing',\n",
       " 'additionally description say comforter',\n",
       " 'comforter receive thing',\n",
       " 'tie duvet cover corner insert still disappoint',\n",
       " 'overall really feel much different comforter college buy real',\n",
       " 'work though',\n",
       " '',\n",
       " 'opportunity comforter full range temperature hope work',\n",
       " 'nonetheless far satisfied comforter',\n",
       " 'appear well make light fit well standard queen size mattress',\n",
       " 'also fit duvet cover well',\n",
       " 'also praise seller',\n",
       " 'comforter ship immediately arrive quickly well packed',\n",
       " 'great transaction way excellent buy',\n",
       " 'buy daughter take college',\n",
       " 'go twin bed need new comforter',\n",
       " 'little skeptical price low come',\n",
       " 'fluffed right take bag tick nice soft',\n",
       " 'size could little big queen fit okay',\n",
       " 'would generous full',\n",
       " 'overall good value price',\n",
       " 'fluffy soft great value',\n",
       " 'quickly damage stain weird smell take packaging',\n",
       " 'keep warm heat definitely good purchase alternative',\n",
       " 'update still love comforter warm light enough summer night still recommend original review think base review order bed',\n",
       " 'full sized sheet make bed extra soft comfy',\n",
       " 'let know arrive update comforter week love super warm heavy soft luxurious prior review son face plant time go lie son often wind sleep perfect buy duvet wash update problem recommend want warm comforter feel heavy think',\n",
       " 'happy comforter',\n",
       " 'come really fast',\n",
       " 'weight perfect season',\n",
       " 'great buy',\n",
       " 'soft',\n",
       " 'little thin appear',\n",
       " 'hot live pretty hot place nice',\n",
       " 'overall really nice',\n",
       " 'description comforter clearly title',\n",
       " 'mean would insert duvet cover previous buy tab suppose anchor stay place reason select',\n",
       " '',\n",
       " 'buy comforter new apartment love right weight heavy light',\n",
       " 'cover go',\n",
       " 'fit cover perfectly also',\n",
       " 'would recommend product',\n",
       " 'love product',\n",
       " 'read gazillion review',\n",
       " 'duvet perfect cost',\n",
       " 'light duvet perfect need really heavy bedding layer light quilt decorative purpose make perfect cold night',\n",
       " 'soft touch fluffy lightweight',\n",
       " 'love fabulous cant_wait see go new cover order great product great price',\n",
       " 'receive new comforter mail today',\n",
       " 'come fast damage',\n",
       " 'expect comforter lot fluff',\n",
       " 'kind upset cover cotton stuff',\n",
       " 'course fault read description properly though',\n",
       " 'debate comforter opt cheap',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'choose',\n",
       " 'may return bother',\n",
       " 'maybe night sleep change mind right disappoint',\n",
       " 'really expect',\n",
       " 'get comforter white sheet look pretty together picture blanket much cushier thick',\n",
       " 'think big guest blanket',\n",
       " 'plush thick hope',\n",
       " 'love',\n",
       " 'exactly expect want',\n",
       " '',\n",
       " 'soft',\n",
       " 'exactly size say',\n",
       " 'warm heavy',\n",
       " 'perfectly hope base dimension product description',\n",
       " 'great buy exactly want',\n",
       " 'comforter good quality shock considering duvet microsuede miserable purchase',\n",
       " '',\n",
       " '',\n",
       " 'sheet softest comfy sheet ever use',\n",
       " 'soft thin enough feel much use love set sturdy',\n",
       " 'color exactly show order page',\n",
       " 'husband like well egyptian cotton sheet thing say oversized',\n",
       " 'order would probably fit king larger',\n",
       " 'really sheet though',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'make really hard leave bed morning make nap time wonderful',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'soft',\n",
       " 'love sheet',\n",
       " 'soft',\n",
       " 'problem pille wash thus far',\n",
       " 'color exactly picture happy see',\n",
       " 'soft',\n",
       " 'love comforter',\n",
       " '',\n",
       " '',\n",
       " 'warm cozy great even warm weather',\n",
       " '',\n",
       " '',\n",
       " 'feel lovely feel envelop go nice warm',\n",
       " 'make soft wash night climb bed love sheet',\n",
       " 'read review prior purchase concerned accuracy thread_count',\n",
       " 'extremely satisfied',\n",
       " 'softest sheet ever own',\n",
       " 'plan buy',\n",
       " 'mattress sheet fit perfectly even wash dry',\n",
       " 'survive wash dry perfectly',\n",
       " 'side ivory little dark picture',\n",
       " 'however happy sheet order',\n",
       " 'soft',\n",
       " 'lot money blow comforter read review took_chance',\n",
       " '',\n",
       " 'keep warm aesthetically pleasing feel nice touch make happy bed',\n",
       " 'price favor get',\n",
       " 'order queen small short side',\n",
       " 'comforter perfect',\n",
       " 'fit perfectly inch floor',\n",
       " 'fighting comforter heavy',\n",
       " 'light yet fluffy definitely warm enough',\n",
       " 'great buy',\n",
       " 'see ikea price range look cheap',\n",
       " 'really nice',\n",
       " 'moment use easily',\n",
       " '',\n",
       " 'know negative review comforter amazing warm top blanket sheet keep warm night quite fluffy alternative comforter plushy fabulous happy decide buy amazing',\n",
       " 'buy comforter year old son',\n",
       " 'want spend lot comforter',\n",
       " 'sure would accident night think try clean soak feather comforter much',\n",
       " 'thought would perfect',\n",
       " 'expectation low',\n",
       " 'long duvet cover sized correctly bed go keeper',\n",
       " 'fast forward couple week can even begin describe impressed comforter',\n",
       " 'son issue move big boy bed sleep room night',\n",
       " 'surprise comforter amazingly warm comfortable cozy',\n",
       " 'love manage accident night easy clean',\n",
       " 'fact love comforter much order king size bedroom',\n",
       " 'highly recommend',\n",
       " 'well',\n",
       " 'product warm thin',\n",
       " 'however need comforter weather cold keep look comforter',\n",
       " 'live disappoint product',\n",
       " 'much thin warm comforter already',\n",
       " 'expect warm definitely',\n",
       " 'warm reviewer describe',\n",
       " 'love comforter',\n",
       " 'soon husband allergy order alternative comforter love warm bulky',\n",
       " 'really great product price warm cozy',\n",
       " 'search bedding month comfortor expensive decide go alternative',\n",
       " 'find price agree like review part',\n",
       " 'place order come next day super excited get home see new warm comforter',\n",
       " '',\n",
       " 'open thinner expect night cold breathable',\n",
       " 'shell traditional blanket synthetic_fiber',\n",
       " 'worth purchase guess search exactly',\n",
       " 'nothing real thing baby',\n",
       " 'loved comforter',\n",
       " 'quite soft well make',\n",
       " 'look pretty small bag come open shake wash comforter fluff right',\n",
       " 'problem shipping',\n",
       " 'box send smash tear',\n",
       " 'buy much love first',\n",
       " 'wash time still amazing good kind weather honestly put duvet',\n",
       " 'sleep soft good product update review',\n",
       " 'buy',\n",
       " 'stay hotel use duvet cover love feel',\n",
       " 'live need thick fill pick think summer heat',\n",
       " 'get yesterday thinking may thick winter',\n",
       " 'wrong last night room large window sleep open almost sleep nice',\n",
       " 'surprised really thick fluffy look picture',\n",
       " 'maybe pack time fluff yet',\n",
       " 'way pleased price could beat',\n",
       " 'would pay full price think full price would less thrilled feel get money worth',\n",
       " 'far good',\n",
       " 'buy bed',\n",
       " 'quality size duvet perfect',\n",
       " 'go buy king size bed',\n",
       " 'can beat price machine_washable amazing alternative duvet',\n",
       " 'love comforter',\n",
       " 'light weight soft warm',\n",
       " 'excellent comforter',\n",
       " 'price unbelieveable',\n",
       " 'thing good true fabulous deal',\n",
       " 'softest cool alternative felt price unbeatable',\n",
       " 'probably go cost find much better worth extra price',\n",
       " 'budget brainer',\n",
       " 'great price',\n",
       " 'place shop new apartment',\n",
       " 'need new comforter much research find',\n",
       " 'look quality alternative comforter avoid allegie eligible prime',\n",
       " 'bed night please comfortable',\n",
       " 'previous comforter near year old fantastic change',\n",
       " 'buy however plan full bed rather',\n",
       " 'end buy queen regret purchasing king size large full queen size comforter',\n",
       " 'comforter barely run size bed inch run',\n",
       " 'toss turn blanket hogger bed may bit small width',\n",
       " 'put shabby chic full queen duvet cover perfectly contimplate purchase new duvet work',\n",
       " 'price great comforter perfect need fluff',\n",
       " 'sheet far exeede expectation',\n",
       " 'nice reasonable price',\n",
       " 'buy mattress make king size bed product fit perfectly',\n",
       " 'highly recommend great buy',\n",
       " 'hesitant comforter read mixed review',\n",
       " 'far exceeded expectation buy consider buy',\n",
       " 'comforter',\n",
       " 'amazingly soft perfect alternative comforter price unbelievable',\n",
       " 'definitely buy need one',\n",
       " '',\n",
       " 'even read poor review opt take chance item',\n",
       " 'arrive quickly instantly disappoint thin remove packaging',\n",
       " 'bad review come true',\n",
       " '',\n",
       " '',\n",
       " 'thought',\n",
       " 'put comforter duvet cover look amazing bed',\n",
       " 'fit great enough fluff',\n",
       " 'keep unbelievably warm night want sized comforter pay',\n",
       " 'excellent price receive opinion worth crisp clean',\n",
       " 'comforter seem hold quite well',\n",
       " 'use month fill bunch corner comforter replace',\n",
       " 'really worth price',\n",
       " 'still comforter still love warm',\n",
       " 'bought sheet part replace set cat shred sharpen claws side bed',\n",
       " 'sheet eventually survive claws odis hold well',\n",
       " 'comfortable look great',\n",
       " 'idea many thread need good sheet',\n",
       " 'good sheet',\n",
       " 'thick sheet',\n",
       " 'love pbteen',\n",
       " 'always want duvet cover pbteen bit expensive',\n",
       " 'full queen',\n",
       " 'find beautiful duvet cover sale site half original price',\n",
       " 'start insert',\n",
       " 'full duvet measurement pbteen find measurement guess purpose buy find measurement',\n",
       " 'hard refuse',\n",
       " 'order',\n",
       " 'arrive next day even select day shipment look take packaging fluff nicely',\n",
       " 'arrive used corner_tab helpful tie cover insert',\n",
       " 'minute put bed',\n",
       " 'fabulous even tell inch difference soft hot warm hot',\n",
       " 'live right night degree use thin quilt',\n",
       " 'probably would better quilt',\n",
       " 'great looking duvet insert definitely get super comfortable warn reviewer say bit edit still love duvet insert warm eventually remove quilt bed fine plan keep take bed longer',\n",
       " 'come bottom mattress',\n",
       " 'thick mattress probably deep bed may bit longer',\n",
       " 'previously write hot modify mean get hot point kick burn much',\n",
       " 'comfortable quilt quilt style warmth trust want warm quilt last thing would',\n",
       " 'still recommend duvet insert get really cold live may want fleece blanket keep comfortable thought would give update',\n",
       " 'must buy look duvet budget',\n",
       " 'comforter warm fit bed well',\n",
       " 'know',\n",
       " 'warm comfortable',\n",
       " 'exactly think would think comforter next good thing',\n",
       " 'love warm fit cover perfectly perfect',\n",
       " 'review would give shot',\n",
       " 'happy',\n",
       " 'soft',\n",
       " 'need deep pocket fit sheet fit great',\n",
       " 'case bs either',\n",
       " '',\n",
       " 'fit large tempurpedic pillow fine',\n",
       " 'good buy',\n",
       " 'girl work jealous tell much spend',\n",
       " 'nice soft stuff',\n",
       " 'sure alternative go cut',\n",
       " 'fluffy',\n",
       " 'love love love',\n",
       " 'comforter perfect way far concern glad remind',\n",
       " 'word need',\n",
       " 'buy buy other full winter sofa blanket',\n",
       " 'great quality fit size bed great',\n",
       " 'comforter several time original review still new',\n",
       " 'comforter somewhat lightweight prefer way work',\n",
       " 'want',\n",
       " 'get due hotel simali one say still fluffy heavy plus blanket heavy tend hot night even fan',\n",
       " 'love blanket would say could little bit fluffy pay dollar',\n",
       " 'would recommend comforter look',\n",
       " 'wait find comfort look white love',\n",
       " 'surprised soft sheet',\n",
       " 'also fit really well fight corner fit bottom',\n",
       " 'easy put sleep',\n",
       " 'hit sheet sooooo soft',\n",
       " 'comforter arrive time perfect condition',\n",
       " 'day put right away',\n",
       " 'comforter fluff wonderfully wash drying',\n",
       " 'perfect weight moderate climate cool cold fall winter night',\n",
       " 'purchase guest happy purchase',\n",
       " '',\n",
       " 'buy false blanket rather true comforter due occasionally gross dog sleep bed',\n",
       " 'apparently good quality rather warm seem last',\n",
       " 'blanket thick live western winter require several layer poverty level either',\n",
       " 'certainly worth money',\n",
       " 'enjoy blanket far',\n",
       " 'need new white comforter read review see good price decide',\n",
       " 'overall good quality warm comforter',\n",
       " 'thickness thick super thin soft',\n",
       " 'get little warm usually end floor end night perfect cold winter night soft warm good value price',\n",
       " 'happy sheet',\n",
       " 'use twice find soft lightweight good way',\n",
       " 'perfectly mattress',\n",
       " 'soft comfy sheet',\n",
       " 'item exactly say nice smooth alternative blanket',\n",
       " 'happy delivery fast free shipping',\n",
       " 'package nice blanket lol',\n",
       " 'size right point',\n",
       " 'exactly hope blanket',\n",
       " 'exactly',\n",
       " 'hold great',\n",
       " 'thing would like thickness',\n",
       " 'overall great money pay',\n",
       " 'great product',\n",
       " 'receive yesterday husband really happy snuggle comforter last night',\n",
       " 'really surprised little pay',\n",
       " 'soon open wash dry well pille loss stuffing',\n",
       " 'feel good quality nice weight fit bed perfectly',\n",
       " 'cover feel fabric would feel nice use duvet intent',\n",
       " 'hesitate recommend want alternative comforter medium weight',\n",
       " 'comforter',\n",
       " 'buy comforter college age son',\n",
       " 'say extremely comfortable would highly recommend duvet cover',\n",
       " 'great price',\n",
       " 'switch double bed queen buy new bedding',\n",
       " 'good comforter good price perfect weight',\n",
       " 'comfortable',\n",
       " 'best ever',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'description right good product',\n",
       " 'light pass terribly thick',\n",
       " 'still nice bed',\n",
       " 'description right good product',\n",
       " '',\n",
       " 'sheet great',\n",
       " 'soft keep warm night',\n",
       " 'likely take summer may little heavy heat great cooler season',\n",
       " 'soft',\n",
       " 'climate',\n",
       " 'heavy warm snugly',\n",
       " 'happy purchase',\n",
       " 'thank',\n",
       " 'buy main bed cover expect really puffy',\n",
       " 'buy replace fiber fill comforter use blanket',\n",
       " 'keep warm light great combination',\n",
       " 'great warmth weight',\n",
       " 'first type purchase look affordable read many review',\n",
       " 'consider order place ship hurricane make way shipment time full queen comforter seem little thin initially expect quite soft',\n",
       " 'warm enough chilly autumn day use blanket winter weather begin bedroom icebox',\n",
       " 'perhaps downfall choose alternative instead actual goose sacrifice little warmth comforter thickness avoid poke face goose feather',\n",
       " 'questionable issue first put comforter bed see black smudge freshly wash blanket put notice black smudge comforter',\n",
       " 'smudge appear spot wash away washcloth mild soap',\n",
       " 'comforter cover tie comforter help greatly duvet cover size duvet set purchase incorrectly list',\n",
       " 'comforter fit full size bed nicely',\n",
       " 'come month may buy slightly small duvet cover extra fluffiness consider bring college campus next year',\n",
       " 'pile big puffy blanket twin size campus bed would feel sleep cloud',\n",
       " 'plus minus',\n",
       " 'experience excellent usual',\n",
       " 'comforter happy',\n",
       " 'warm well make',\n",
       " 'worth',\n",
       " 'excellent comforter',\n",
       " 'beat price product',\n",
       " 'great comforter especially dog owner dog destroy real goose item',\n",
       " 'nice price',\n",
       " 'surprised great quality sheet price',\n",
       " 'extremely soft comfortable stitching sturdy well make',\n",
       " 'also please easily cal mattress hassle',\n",
       " 'buy set different color happy',\n",
       " 'much better expect',\n",
       " 'awesome comforter price',\n",
       " 'would pay way get mail',\n",
       " 'ship fast exceeded expectation could think fall',\n",
       " 'sure warm summer time',\n",
       " 'soft enough fluff flat',\n",
       " 'live climate may warm enough',\n",
       " 'great buy',\n",
       " 'product live review',\n",
       " 'thin look okay',\n",
       " 'expensive super high quality comforter also mid range one',\n",
       " 'low end',\n",
       " 'bat double bat seam seam thin sheet comforter warm',\n",
       " 'shiver go old comforter hope replace',\n",
       " 'maybe good blanket people live warm climate tend run hot temperature wise maybe people ancestor cold northern mountain wear short winter comfortable',\n",
       " 'tend cold winter wear glove seek heat',\n",
       " '',\n",
       " '',\n",
       " 'want blanket protect weather keep warm',\n",
       " 'disappointing',\n",
       " 'fluffiness comforter comforter',\n",
       " 'flat',\n",
       " 'box come small flat say soon get know way full queen comforter quality could stuffed small bag',\n",
       " 'cold thin low quality material',\n",
       " 'buy low price',\n",
       " 'track price item thinking buying day',\n",
       " 'finally pay surprisingly find low price stock',\n",
       " 'make feel warm night live really cold day',\n",
       " 'warm deserve',\n",
       " 'year old buying first comforter must say suitable match warmth comfort budget need',\n",
       " 'problem comforter know retain water',\n",
       " 'good great warm comforter',\n",
       " 'couple week',\n",
       " '',\n",
       " '',\n",
       " 'thin soft price worth',\n",
       " 'sheet probably high quality sooo soft',\n",
       " 'place order second set sheet',\n",
       " 'believe price far good set sheet own year life mattress',\n",
       " 'pay well sized sheet never fit sheet properly fit double pillow top mattress',\n",
       " 'sheet actually fit high quality amazing price',\n",
       " 'buy brand',\n",
       " 'price good amaizing quality sleep cloud well expectation amaize quality sleep cloud',\n",
       " 'buy blanket year old',\n",
       " 'blanket heavy hot thin',\n",
       " 'say good blanket ever',\n",
       " 'say hard',\n",
       " 'cozy blanket',\n",
       " 'look non heavy still warm comforter light fluffy could ask',\n",
       " 'light fluffy',\n",
       " 'quality sheet amazing super soft',\n",
       " 'would highly recommend',\n",
       " 'would rate star color slightly picture online gray sheet almost look',\n",
       " 'great sheet',\n",
       " 'order gold full size sleep number bed say extremely nice color spot picture sheet smooth soft lay silk fit full size mattress nicely can beat price selling amazing could charge lot consider nice beautiful soft bed sheet',\n",
       " 'take awhile choose sheet could feel material',\n",
       " 'material soft silky perfect',\n",
       " 'warm climbing bed',\n",
       " 'color exactly show',\n",
       " 'sheet fit matress perfect deep matress sure would fit perfect stretch',\n",
       " 'sheet available color cost much',\n",
       " 'sure go receive quality base seller price',\n",
       " 'satisfied purchase would purchase also sheet wash time color fade bleed',\n",
       " 'show wear washing either',\n",
       " 'highly recommend sheet way better expect',\n",
       " 'comforter warm wonderful',\n",
       " 'buy bed would fit generously',\n",
       " 'would highly recommend comforter good',\n",
       " 'order daughter send wrong color',\n",
       " 'want disapointe time',\n",
       " 'order color want different company',\n",
       " 'return address',\n",
       " 'pay return item wrong color ugh ugh order form say sure',\n",
       " 'right item wrong color',\n",
       " 'comforter soft comfy would like little fluffy regardless great price',\n",
       " '',\n",
       " 'exactly looking feel great fluffy soft',\n",
       " 'fit bed nicely',\n",
       " '',\n",
       " 'ship promptly',\n",
       " 'keep',\n",
       " 'goose nice cuddly alternative definately suitable runner',\n",
       " 'soft fluffy warm',\n",
       " 'warm goose replace',\n",
       " 'fact warm sleep hot',\n",
       " 'people would think comforter would comfortable temperature wise',\n",
       " 'trouble duvet',\n",
       " 'settle well',\n",
       " 'well',\n",
       " 'short price get seem great far',\n",
       " 'soft enough worth money',\n",
       " 'incredible buy',\n",
       " 'want hot would still keep warm night drop temp',\n",
       " 'perfect',\n",
       " 'amazing value',\n",
       " 'exactly need bedroom',\n",
       " 'fit well duvet cover',\n",
       " 'heavy thin',\n",
       " '',\n",
       " 'order',\n",
       " 'seller put label describe say king include dimension',\n",
       " 'order set incorrect',\n",
       " 'money exchange seller',\n",
       " 'frustrating sheet soft good price',\n",
       " '',\n",
       " 'base seller seem careless',\n",
       " 'mislabeled package equal wrong size',\n",
       " 'weary great price wonderful deep mattress deep pocket make sheet fit perfectly soft come wash perfectly pille wrinkle',\n",
       " 'buy set fabulous',\n",
       " 'sheet soft',\n",
       " 'other thick call thin',\n",
       " 'overall incredible value price',\n",
       " 'nice sheet price',\n",
       " '',\n",
       " 'read review prior purchasing sheet set',\n",
       " 'almost choose strong negative review',\n",
       " 'decide try anyway husband talk count thread sheet worker tell much better feel',\n",
       " 'shop different home store course price far willing pay',\n",
       " 'went_ahead bit bullet thinking',\n",
       " 'break husband expect feel cheat',\n",
       " 'washed sheet put husband go',\n",
       " 'get bed night newborn baby mamma arm keep tell good feel ask much spend',\n",
       " 'shocked price tell buy set plan',\n",
       " 'would totally recommend set want good night sleep exceed expectation',\n",
       " 'daughter love comforter much order',\n",
       " 'comfortable look durable well',\n",
       " 'picture comfortable',\n",
       " '',\n",
       " 'love sheet',\n",
       " 'soft keep warm',\n",
       " 'thing thought embroidered_stripe would flat sheet also pillowcase',\n",
       " 'wonderful sheet',\n",
       " 'never really care spend whole lot money sheet',\n",
       " 'sheet sheet',\n",
       " 'expensive one best',\n",
       " 'plan buy soon however wrinkle otherwise great',\n",
       " 'soft',\n",
       " 'sheet slight odor receive washing take care',\n",
       " 'sheet soft great bargain',\n",
       " 'sheet',\n",
       " 'love month happy wash couple time stuff shift',\n",
       " 'little thing get little bit stuff cling',\n",
       " 'washing stuffing go',\n",
       " 'happy purchase',\n",
       " '',\n",
       " 'incredibly soft light comfortable comforter',\n",
       " 'similar item pay time much',\n",
       " 'see hold buy future problem',\n",
       " 'nice soft warm',\n",
       " 'read review decide',\n",
       " 'full queen comforter seem much thin warm expect disappoint',\n",
       " 'disappointed',\n",
       " 'comforter great replacement old wear flat feather insert',\n",
       " 'look feather bed feel definitely right insert',\n",
       " 'may feel feather soft fluffy warm',\n",
       " 'old feather bed never comfortable insert even newly purchase',\n",
       " 'live keep home cool night second blanket stay completely warm thing feather insert well',\n",
       " 'want point order king sized comforter sized cover due review say little small',\n",
       " 'sizing work perfect give product star exceed expectation',\n",
       " 'great product price',\n",
       " 'get fill duvet',\n",
       " 'thought may warm enough ggrreeaatt light weight plenty warm well price easy tie duvet',\n",
       " 'corner_tab work great',\n",
       " 'perfect great alternative comforter',\n",
       " 'warm comforter',\n",
       " 'love feather deal change sheet',\n",
       " 'feel soft even husband surprised quality comforter price',\n",
       " 'highly recommend',\n",
       " 'try wash though',\n",
       " 'say hold',\n",
       " 'warm comforter',\n",
       " 'feel wonderful hotel bed duvet cover soft fluffy comfy great price arrive kind flat puff really nicely',\n",
       " 'love great comforter',\n",
       " 'nice warm',\n",
       " 'soft smooth',\n",
       " 'size large enough drape side deep king bed',\n",
       " 'comfortable',\n",
       " 'buy mom love',\n",
       " 'elderly always cold',\n",
       " 'perfect gift warm comfy',\n",
       " 'may expectation high due positive review',\n",
       " 'comforter feel extremely light fill cotton warm feel high quality',\n",
       " 'look good unfortunately make good comforter',\n",
       " 'return get high quality comforter',\n",
       " 'disappointed',\n",
       " 'order decorate room granddaughter',\n",
       " 'deliver fast quality great weight perfect',\n",
       " 'pleased comforter would definitely recommend other',\n",
       " '',\n",
       " 'much imagination',\n",
       " 'keep warmth thick comfortable think good price product worth would like recommand friend',\n",
       " 'good product',\n",
       " 'month keep warm toasty bulk bed cozy',\n",
       " 'room comforter warm enough keep freeze night warm overheat',\n",
       " 'also super bulky pretty lightweight lift maneuver duvet cover',\n",
       " 'perfect need nighttime temperature degree',\n",
       " 'wash assume wash fine',\n",
       " 'perfect amount warmth bulk',\n",
       " 'order heavy full feather land closet bad trash',\n",
       " 'comforter light weight warm need color change cover thingy',\n",
       " 'much',\n",
       " 'lot write seem well make',\n",
       " 'relatively soft good color quality',\n",
       " 'nice quality good color',\n",
       " '',\n",
       " 'comforter warm real',\n",
       " 'wrap cloud',\n",
       " 'come quickly really love',\n",
       " 'fit full size platform bed beautifully',\n",
       " 'wrap cloud',\n",
       " 'buy guest room get cold rest hous winter',\n",
       " 'stay overnight ask get',\n",
       " 'love fact warm heavy warm',\n",
       " 'great medium weight comforter',\n",
       " 'heavy able keep year heavy enough keep cozy warm cold winder night',\n",
       " 'take day fluff come life package',\n",
       " 'great reuseable cover store blanket',\n",
       " 'good price',\n",
       " 'good',\n",
       " 'set',\n",
       " 'guest room already compliment guest soft comfortable sheet soft',\n",
       " 'sheet great nice deep pocket fit sheet',\n",
       " 'color pic',\n",
       " 'high theead count make sheet soft',\n",
       " 'however high also make warm sleep hot',\n",
       " 'really good thing say bed',\n",
       " 'great soft sheet',\n",
       " 'get change one also get duvet love work perfect recomende love',\n",
       " 'thick last plush soft',\n",
       " 'light warm would describe',\n",
       " 'adquately cover check actual size fool word',\n",
       " 'soft fluffy',\n",
       " 'comfortable light sheet great price',\n",
       " 'fit sheet actually fit foam mattress lot fit sheet',\n",
       " '',\n",
       " 'love sheet',\n",
       " 'come wonderful',\n",
       " 'would highly recommend sheet',\n",
       " 'wonderful',\n",
       " 'get sale hope would high quality',\n",
       " 'mom order disappointed favorite sheet',\n",
       " 'hot soft skin',\n",
       " 'soft comfy',\n",
       " 'wish separate size queen full enough overflow cover side mattress',\n",
       " 'love quality comforter wish little large',\n",
       " 'would like',\n",
       " '',\n",
       " 'took_chance buy non goose comforter',\n",
       " 'surprised loftiness material soft almost slick feeling material soon unzipped comforter want expand',\n",
       " 'nice tight cover corner_tab tie help hold place duvet',\n",
       " 'warmth weight many blanket night fall bed great money save alternative',\n",
       " 'terrific sheet extremely soft luxurious',\n",
       " 'use may little hot',\n",
       " 'super soft kinda hot',\n",
       " 'friend buy sheet tell soon feel know get especially price',\n",
       " 'super soft sheet',\n",
       " 'think sheet still super soft sure cost lot',\n",
       " 'friend buy blue color beautiful exactly color look online fit bed exactly',\n",
       " 'buy set sheet gold camel color give gift beautiful color bed',\n",
       " 'tear end choose color glad',\n",
       " 'beige color look really ugly imo online person much pretty',\n",
       " 'go perfectly bed',\n",
       " 'also order instead white sheet really soft also kind thin',\n",
       " 'pattern would show get white sheet',\n",
       " 'pattern barely noticeable beige sheet',\n",
       " 'still glad order',\n",
       " 'good deal ever get sheet',\n",
       " 'pay much money sheet nearly soft',\n",
       " 'family extra money go gift buy extra set bed guest bedroom',\n",
       " 'color see far beautiful sheet feel good sleep husband also love definitely notice difference',\n",
       " 'definitely recommend sheet afraid order color much pretty person',\n",
       " 'month update month use sheet wash feel need take star review',\n",
       " 'still buy still find thin ness somewhat problematic',\n",
       " 'wash pretty frequently make sheet round bed',\n",
       " 'set sheet lot sturdier set never problem sheet cheap set buy year ago',\n",
       " '',\n",
       " '',\n",
       " 'nearly soft real cotton low thin',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'sheet bunch fit sheet stay mattress',\n",
       " 'night sleep sheet come mattress bunch feel need rip bed wash send bottom bedsheet',\n",
       " 'can imagine sheet mutliple pair sheet sheet',\n",
       " '',\n",
       " '',\n",
       " 'sheet various well make well sheet think especially high one get good deal find never problem',\n",
       " '',\n",
       " '',\n",
       " 'ever',\n",
       " '',\n",
       " '',\n",
       " 'super cheap pair get',\n",
       " '',\n",
       " '',\n",
       " 'want knock sheet still love definitely think take time read review purchase need know perfect',\n",
       " 'still stand original review edit living sheet month find quite perfect thought',\n",
       " 'also buy set',\n",
       " 'would buy current price write review knowing know',\n",
       " 'still good deal sure spend money',\n",
       " 'feel super soft really bothersome stay bed',\n",
       " '',\n",
       " '',\n",
       " 'initial purchase review month update',\n",
       " '',\n",
       " 'read alternative description',\n",
       " 'heavy hot word goose anywhere description',\n",
       " 'apocalypse',\n",
       " 'great',\n",
       " 'would comfortable cover',\n",
       " 'warm perfect weight heavy enough feel enough weigh foot know mean',\n",
       " 'would purchase size match bed',\n",
       " 'day use winter weather',\n",
       " 'wonderful',\n",
       " 'comforter really nice warm expect would',\n",
       " 'problem order',\n",
       " 'take week get deliver people stair unit instead live upstairs',\n",
       " 'people send comforter',\n",
       " 'enjoy much',\n",
       " 'comforter',\n",
       " '',\n",
       " 'sheet stay bed great say describe breathable sheet warm',\n",
       " 'describe',\n",
       " 'perfect weight',\n",
       " 'husband like well',\n",
       " 'thing may preferred color great sleeping',\n",
       " 'hold',\n",
       " 'would buy recommend friend',\n",
       " 'know else say good purchase',\n",
       " 'product come fast beautiful',\n",
       " 'price right material light',\n",
       " 'really',\n",
       " 'great',\n",
       " '',\n",
       " 'dosent spesefecathion',\n",
       " '',\n",
       " '',\n",
       " 'prise qulity',\n",
       " 'like',\n",
       " 'thin',\n",
       " 'bad color',\n",
       " 'light blue color look old neon',\n",
       " 'return unattractive',\n",
       " 'thankful easy return',\n",
       " 'get nice cream_colored sheet instead could look color feel purchase',\n",
       " 'thing hard buy online',\n",
       " 'thin cheap',\n",
       " 'pay',\n",
       " '',\n",
       " 'usually purchase sheet go store touch recommend friend',\n",
       " 'soft cozy cuddly',\n",
       " 'cover sheet keep warm season',\n",
       " 'color spot show photo well',\n",
       " 'love',\n",
       " '',\n",
       " 'alot people say soft sheet cheeply make get pay also say remove',\n",
       " '',\n",
       " 'never buy alternative comforter',\n",
       " 'gift',\n",
       " 'thick hope warm enough',\n",
       " 'pay happy receive',\n",
       " 'decent price',\n",
       " 'great feeling comforter',\n",
       " 'great price think go light cheap feeling',\n",
       " 'look feel great',\n",
       " 'comforter',\n",
       " 'product however think would thicker heavy weight',\n",
       " 'good comforter',\n",
       " 'nice product',\n",
       " 'comforter super warm',\n",
       " 'product expect tee',\n",
       " 'full sure fit size bed',\n",
       " 'would recommend comforter great buy',\n",
       " 'comforter comfortable feeling',\n",
       " 'never lint feather come comforter',\n",
       " 'comforter month hold even several wash',\n",
       " 'get package upon_opening comforter bag part fully closed comforter appear little dirty area',\n",
       " 'come right',\n",
       " 'comforter little less fluffy expect heavy keep warm',\n",
       " 'pleased comforter price product overall good transaction',\n",
       " 'great',\n",
       " 'comforter high quality lightweight well construct warm nice look great value',\n",
       " 'highly recommend',\n",
       " 'work well size bed give coverage people bed',\n",
       " 'great comforter great value',\n",
       " 'decide buy sheet reviewer comment sheet soft indeed super soft',\n",
       " 'egyptian comment similar item manufacturer',\n",
       " 'bother comfortable',\n",
       " 'buy size sheet fit mattress well color true picture',\n",
       " 'prior put bed seem hold fine',\n",
       " 'reason give star ridiculously soft sheet seem fairly thin sure long hold buy sheet often',\n",
       " 'however price really beat find store queen size sheet soft less',\n",
       " 'would definitely buy',\n",
       " 'super soft',\n",
       " 'supplier amazingly fast sheet come well package',\n",
       " 'impressed get quality outstanding price',\n",
       " 'wife equate quality big price past',\n",
       " 'surprise find good quality great price',\n",
       " 'concerned people produce product',\n",
       " 'long hour low pay sometimes feel get good deal',\n",
       " 'certainly hope retailer certain standard production environment meet',\n",
       " 'would pay little assure happen',\n",
       " 'maybe enjoy great product help pay rent food',\n",
       " 'wish could great comfort manufacturer provide family',\n",
       " 'sheet match rest bed',\n",
       " '',\n",
       " 'thickness feel sheet',\n",
       " 'seem bit thin punctuate animal get bed screw nice new sheet even sleep',\n",
       " 'price',\n",
       " 'much nice crap get price point',\n",
       " 'make sure keep animal away',\n",
       " 'hold well may hope',\n",
       " 'plan purchase set think probably comfortable sheet sleep year',\n",
       " 'least own',\n",
       " 'great price',\n",
       " '',\n",
       " 'sheet soft thin',\n",
       " 'overall think quality pretty good considering price pay item',\n",
       " 'would definitely recommend purchase sheet sheet soft care thickness quality fabric',\n",
       " 'good product price',\n",
       " 'set money wish pillow case instead still set great fit',\n",
       " 'order delighted',\n",
       " 'would hesitate tell oroder need twin comfort great price',\n",
       " 'make',\n",
       " 'great selection bed',\n",
       " 'get set try',\n",
       " 'think',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'hmmmmm wonder good true',\n",
       " 'get first set wash put bed',\n",
       " 'soft nice fit sheet fit mattress great',\n",
       " 'went_ahead buy set parent bed guest bed',\n",
       " 'parent love',\n",
       " '',\n",
       " 'sell extra pillow case pillow bed many sheet set swap set',\n",
       " 'follow promotion discount item spend order',\n",
       " 'reassure entitled discount place order would forward request',\n",
       " 'couple day inform would give discount',\n",
       " 'happy sheet promotion tell qualify',\n",
       " 'heavenly',\n",
       " 'slept quality goose comforter gift',\n",
       " 'much difference dollar goose comforter',\n",
       " 'little crinkly sound move cover get bit warm goose feel nearly identical',\n",
       " 'pleased price performance buy',\n",
       " '',\n",
       " 'would buy time time sleep cloud would recomend look sheet',\n",
       " '',\n",
       " 'split receive product gift purchase',\n",
       " 'bed new order set first time wash product hem pillowcase come apart sheet even come close fitting bed',\n",
       " 'wonder new set purchase even get box',\n",
       " 'sheet soft look',\n",
       " 'wonder sheet hold washing',\n",
       " 'bed mattress',\n",
       " 'say sheet green gift set camel color new one',\n",
       " 'sheet make fit elastic corner sheet apx set',\n",
       " 'bad fit',\n",
       " 'go cheapo wally world sheet holy cow difference',\n",
       " 'compliment',\n",
       " 'sheet connoisseur would describe luxurious',\n",
       " 'week wash exactly comment durability hold far string hang',\n",
       " 'like much order set bed house',\n",
       " 'great far',\n",
       " '',\n",
       " 'nice light last heavy',\n",
       " 'work really well think provide heat need warm cool weather',\n",
       " 'great',\n",
       " 'joy receive product exactly advertise',\n",
       " 'winter night goose alternative perfect',\n",
       " 'keep body heat yet heavy',\n",
       " 'appearance',\n",
       " '',\n",
       " '',\n",
       " 'simply white',\n",
       " 'perfect',\n",
       " 'soft luxurious feeling thick would expect',\n",
       " 'would definitely recommend other',\n",
       " 'love sheet',\n",
       " 'excellent value price',\n",
       " 'much soft cotton sheet',\n",
       " 'leary wash instruction launder magnificently dry low setting',\n",
       " 'buy question',\n",
       " 'great value soft silk',\n",
       " 'soft good quality great price love good fit',\n",
       " 'would buy',\n",
       " 'want second set',\n",
       " 'thumb',\n",
       " 'purchase believe thin sheet see awful',\n",
       " '',\n",
       " 'comfy highly recommend',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_lemetised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('list_sentences_lemetised_Sheets.pkl', 'wb') as f:\n",
    "    pickle.dump(list_sentences_lemetised, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comforter'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=3, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.154*\"soft\" + 0.056*\"good\" + 0.044*\"comfortable\" + 0.039*\"nice\" + '\n",
      "  '0.037*\"fit\" + 0.026*\"well\" + 0.024*\"super\" + 0.022*\"purchase\" + '\n",
      "  '0.021*\"really\" + 0.019*\"make\"'),\n",
      " (1,\n",
      "  '0.070*\"great\" + 0.049*\"color\" + 0.044*\"price\" + 0.044*\"quality\" + '\n",
      "  '0.041*\"star\" + 0.033*\"wash\" + 0.026*\"would\" + 0.024*\"get\" + 0.022*\"order\" + '\n",
      "  '0.021*\"look\"'),\n",
      " (2,\n",
      "  '0.202*\"sheet\" + 0.045*\"love\" + 0.041*\"buy\" + 0.038*\"feel\" + 0.037*\"set\" + '\n",
      "  '0.030*\"bed\" + 0.020*\"thin\" + 0.016*\"mattress\" + 0.014*\"product\" + '\n",
      "  '0.013*\"sleep\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -6.105583198452926\n",
      "\n",
      "Coherence Score:  0.2622397872066436\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1783112292508464371410415\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1783112292508464371410415_data = {\"mdsDat\": {\"x\": [-0.377071152053348, 0.17259873315398536, 0.20447241889936277], \"y\": [0.01820806069974175, -0.33221073722194755, 0.3140026765222057], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [40.40120315551758, 31.65384864807129, 27.944950103759766]}, \"tinfo\": {\"Term\": [\"sheet\", \"soft\", \"great\", \"good\", \"color\", \"love\", \"price\", \"quality\", \"comfortable\", \"buy\", \"star\", \"nice\", \"feel\", \"set\", \"fit\", \"wash\", \"bed\", \"would\", \"well\", \"get\", \"super\", \"order\", \"purchase\", \"look\", \"really\", \"thin\", \"make\", \"mattress\", \"deep\", \"material\", \"sheet\", \"love\", \"buy\", \"set\", \"feel\", \"bed\", \"thin\", \"mattress\", \"ever\", \"sleep\", \"product\", \"time\", \"come\", \"pillow\", \"cheap\", \"recommend\", \"go\", \"first\", \"fitted\", \"even\", \"use\", \"size\", \"want\", \"seem\", \"case\", \"expect\", \"new\", \"last\", \"top\", \"return\", \"stain\", \"great\", \"color\", \"price\", \"quality\", \"star\", \"wash\", \"would\", \"get\", \"order\", \"look\", \"review\", \"material\", \"definitely\", \"much\", \"hot\", \"still\", \"far\", \"hold\", \"see\", \"be\", \"little\", \"fabric\", \"pretty\", \"month\", \"value\", \"high\", \"try\", \"think\", \"sure\", \"way\", \"have\", \"soft\", \"good\", \"comfortable\", \"nice\", \"fit\", \"well\", \"super\", \"purchase\", \"really\", \"make\", \"deep\", \"cool\", \"perfect\", \"night\", \"amazing\", \"light\", \"say\", \"also\", \"cotton\", \"happy\", \"beautiful\", \"put\", \"pocket\", \"stay\", \"keep\", \"money\", \"corner\", \"worth\", \"warm\", \"need\"], \"Freq\": [40679.0, 21413.0, 11052.0, 7728.0, 7760.0, 9113.0, 6932.0, 6869.0, 6061.0, 8251.0, 6379.0, 5464.0, 7663.0, 7447.0, 5150.0, 5203.0, 6056.0, 4028.0, 3625.0, 3738.0, 3303.0, 3494.0, 3086.0, 3359.0, 2868.0, 4010.0, 2703.0, 3310.0, 2174.0, 2196.0, 40678.703125, 9112.3740234375, 8250.54296875, 7446.66259765625, 7662.75146484375, 6055.8525390625, 4009.852783203125, 3309.974609375, 2584.5625, 2677.216552734375, 2718.81640625, 2433.085205078125, 2219.380859375, 2002.1365966796875, 2192.9326171875, 1754.6536865234375, 1642.0335693359375, 1597.4185791015625, 1509.4925537109375, 1586.2672119140625, 1586.1246337890625, 1412.8277587890625, 1437.08740234375, 1396.330810546875, 1169.73779296875, 1248.0963134765625, 1200.2508544921875, 1170.0264892578125, 1149.9349365234375, 1089.1737060546875, 1096.479248046875, 11051.5771484375, 7759.58544921875, 6931.52197265625, 6868.81396484375, 6379.2001953125, 5202.72265625, 4027.7314453125, 3738.216796875, 3494.20458984375, 3358.861572265625, 2176.562255859375, 2195.958984375, 1905.354736328125, 1826.7611083984375, 1851.3162841796875, 1586.85009765625, 1615.7835693359375, 1452.0430908203125, 1441.417236328125, 1506.7337646484375, 1253.6455078125, 1286.6148681640625, 1278.33740234375, 1198.471923828125, 1107.731201171875, 1095.46923828125, 1084.2764892578125, 1075.6343994140625, 1072.679931640625, 1031.9180908203125, 1198.156494140625, 21412.521484375, 7727.9169921875, 6061.01708984375, 5463.36181640625, 5150.189453125, 3624.782470703125, 3303.145751953125, 3085.56103515625, 2867.55859375, 2702.804443359375, 2173.68701171875, 1902.5540771484375, 1882.292724609375, 1837.483642578125, 1722.2965087890625, 1686.8609619140625, 1620.02294921875, 1445.7109375, 1432.1417236328125, 1409.89599609375, 1451.284912109375, 1526.11865234375, 1294.6153564453125, 1313.7945556640625, 1209.6461181640625, 1151.6329345703125, 1097.5255126953125, 1107.7301025390625, 949.8837280273438, 1034.9842529296875], \"Total\": [40679.0, 21413.0, 11052.0, 7728.0, 7760.0, 9113.0, 6932.0, 6869.0, 6061.0, 8251.0, 6379.0, 5464.0, 7663.0, 7447.0, 5150.0, 5203.0, 6056.0, 4028.0, 3625.0, 3738.0, 3303.0, 3494.0, 3086.0, 3359.0, 2868.0, 4010.0, 2703.0, 3310.0, 2174.0, 2196.0, 40679.484375, 9113.123046875, 8251.3046875, 7447.4091796875, 7663.56494140625, 6056.61279296875, 4010.645263671875, 3310.771240234375, 2585.31005859375, 2677.996337890625, 2719.629150390625, 2433.84375, 2220.1318359375, 2002.862060546875, 2193.74267578125, 1755.4036865234375, 1642.8038330078125, 1598.168701171875, 1510.2186279296875, 1587.041259765625, 1586.9014892578125, 1413.57421875, 1437.868408203125, 1397.166259765625, 1170.4493408203125, 1248.8746337890625, 1201.00048828125, 1170.79833984375, 1150.6937255859375, 1089.94921875, 1097.2786865234375, 11052.322265625, 7760.33544921875, 6932.26708984375, 6869.56298828125, 6379.91015625, 5203.48974609375, 4028.483642578125, 3738.99609375, 3494.95556640625, 3359.63330078125, 2177.33349609375, 2196.781982421875, 1906.1170654296875, 1827.539794921875, 1852.148193359375, 1587.621337890625, 1616.5689697265625, 1452.8060302734375, 1442.1749267578125, 1507.554931640625, 1254.394775390625, 1287.4012451171875, 1279.119140625, 1199.25244140625, 1108.46826171875, 1096.2109375, 1085.0447998046875, 1076.4010009765625, 1073.4871826171875, 1032.720703125, 1199.3175048828125, 21413.244140625, 7728.68212890625, 6061.73388671875, 5464.10791015625, 5150.91845703125, 3625.52587890625, 3303.8525390625, 3086.301025390625, 2868.305419921875, 2703.553955078125, 2174.39208984375, 1903.2791748046875, 1883.0196533203125, 1838.22119140625, 1723.049560546875, 1687.614501953125, 1620.7921142578125, 1446.4478759765625, 1432.886474609375, 1410.6324462890625, 1452.046142578125, 1526.9202880859375, 1295.3057861328125, 1314.506103515625, 1210.3770751953125, 1152.37841796875, 1098.2567138671875, 1108.50439453125, 950.6067504882812, 1035.7843017578125], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.5973000526428223, -3.093400001525879, -3.192699909210205, -3.2952001094818115, -3.2665998935699463, -3.502000093460083, -3.914299964904785, -4.106100082397461, -4.353499889373779, -4.31820011138916, -4.302800178527832, -4.413899898529053, -4.505799770355225, -4.608799934387207, -4.5177998542785645, -4.740699768066406, -4.80709981918335, -4.83459997177124, -4.891200065612793, -4.841599941253662, -4.841700077056885, -4.957399845123291, -4.940400123596191, -4.969200134277344, -5.146200180053711, -5.081399917602539, -5.120500087738037, -5.145999908447266, -5.163300037384033, -5.217599868774414, -5.210899829864502, -2.656399965286255, -3.0100998878479004, -3.1229000091552734, -3.131999969482422, -3.2060000896453857, -3.4098000526428223, -3.665800094604492, -3.7404000759124756, -3.8078999519348145, -3.847399950027466, -4.281300067901611, -4.27239990234375, -4.414299964904785, -4.456500053405762, -4.4430999755859375, -4.597300052642822, -4.57919979095459, -4.685999870300293, -4.693399906158447, -4.649099826812744, -4.833000183105469, -4.807000160217285, -4.813499927520752, -4.877999782562256, -4.956699848175049, -4.967800140380859, -4.978099822998047, -4.986100196838379, -4.988900184631348, -5.027599811553955, -4.878200054168701, -1.8703999519348145, -2.8896000385284424, -3.132499933242798, -3.236299991607666, -3.2953999042510986, -3.6466000080108643, -3.739500045776367, -3.8076999187469482, -3.8808999061584473, -3.9400999546051025, -4.1579999923706055, -4.291200160980225, -4.3018999099731445, -4.326000213623047, -4.390699863433838, -4.411499977111816, -4.451900005340576, -4.565800189971924, -4.575200080871582, -4.59089994430542, -4.5619001388549805, -4.51170015335083, -4.676199913024902, -4.661499977111816, -4.744100093841553, -4.7932000160217285, -4.841300010681152, -4.832099914550781, -4.985799789428711, -4.900000095367432], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9063000082969666, 0.9061999917030334, 0.9061999917030334, 0.9061999917030334, 0.9061999917030334, 0.9061999917030334, 0.9060999751091003, 0.9060999751091003, 0.906000018119812, 0.906000018119812, 0.906000018119812, 0.906000018119812, 0.906000018119812, 0.9059000015258789, 0.9059000015258789, 0.9059000015258789, 0.9057999849319458, 0.9057999849319458, 0.9057999849319458, 0.9057999849319458, 0.9057999849319458, 0.9057999849319458, 0.9057999849319458, 0.9057000279426575, 0.9057000279426575, 0.9057000279426575, 0.9057000279426575, 0.9057000279426575, 0.9057000279426575, 0.9056000113487244, 0.9056000113487244, 1.1502000093460083, 1.1502000093460083, 1.1502000093460083, 1.1502000093460083, 1.1502000093460083, 1.1502000093460083, 1.1500999927520752, 1.1500999927520752, 1.1500999927520752, 1.1500999927520752, 1.149999976158142, 1.149899959564209, 1.149899959564209, 1.149899959564209, 1.149899959564209, 1.1497999429702759, 1.1497999429702759, 1.1497999429702759, 1.1497999429702759, 1.1497999429702759, 1.1497000455856323, 1.1497000455856323, 1.1497000455856323, 1.1497000455856323, 1.1496000289916992, 1.1496000289916992, 1.1496000289916992, 1.1496000289916992, 1.1496000289916992, 1.1495000123977661, 1.1492999792099, 1.274899959564209, 1.2747999429702759, 1.2747999429702759, 1.2747999429702759, 1.2747999429702759, 1.2747000455856323, 1.2747000455856323, 1.2747000455856323, 1.2747000455856323, 1.2747000455856323, 1.2746000289916992, 1.2746000289916992, 1.2745000123977661, 1.2745000123977661, 1.2745000123977661, 1.2745000123977661, 1.2745000123977661, 1.274399995803833, 1.274399995803833, 1.274399995803833, 1.274399995803833, 1.274399995803833, 1.274399995803833, 1.274399995803833, 1.2742999792099, 1.2742999792099, 1.2742999792099, 1.2741999626159668, 1.2741999626159668, 1.2741999626159668]}, \"token.table\": {\"Topic\": [3, 3, 2, 3, 1, 1, 1, 1, 2, 1, 3, 3, 3, 3, 3, 2, 1, 1, 1, 2, 2, 1, 1, 3, 1, 2, 1, 3, 2, 3, 1, 2, 2, 2, 2, 3, 1, 3, 2, 2, 1, 3, 2, 1, 3, 2, 2, 3, 1, 3, 3, 2, 3, 1, 3, 2, 2, 1, 3, 3, 2, 3, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 3, 1, 2, 3, 2, 3, 2, 1, 2, 1, 1, 2, 1, 2, 1, 3, 2, 2, 3, 3, 2], \"Freq\": [0.9996903538703918, 0.9993909001350403, 0.9996318817138672, 0.9992795586585999, 0.9998988509178162, 0.9999630451202393, 0.9996160864830017, 0.9996614456176758, 0.9999567866325378, 0.9994902014732361, 0.9998789429664612, 0.9998533129692078, 0.9997662305831909, 0.9993813633918762, 0.9998196959495544, 0.9994139671325684, 0.9993438720703125, 0.9998800754547119, 0.9992996454238892, 0.9996883273124695, 0.9996480345726013, 0.9999262690544128, 0.9992687106132507, 0.999821662902832, 0.9991930723190308, 0.999733567237854, 0.9995107054710388, 0.9999117255210876, 0.9999708533287048, 0.9995516538619995, 0.0008338075713254511, 0.9989014267921448, 0.9988953471183777, 0.9994451999664307, 0.9993800520896912, 0.999688446521759, 0.9993181228637695, 0.9996358752250671, 0.9996852874755859, 0.9998114705085754, 0.9998767375946045, 0.9997950792312622, 0.9996440410614014, 0.9997670650482178, 0.9996716380119324, 0.9989556670188904, 0.9997046589851379, 0.9992427825927734, 0.9991669654846191, 0.9997972249984741, 0.9993356466293335, 0.9997265934944153, 0.9994584918022156, 0.9995695948600769, 0.9997639060020447, 0.999125063419342, 0.9999614953994751, 0.9997686743736267, 0.9999024868011475, 0.9993972778320312, 0.9999180436134338, 0.9998935461044312, 0.9997700452804565, 0.9991291165351868, 0.9998468160629272, 0.9995113015174866, 0.99918532371521, 0.9991652965545654, 0.9999450445175171, 0.9999880790710449, 0.9995937943458557, 0.999627947807312, 0.9999886155128479, 0.9988346695899963, 0.9998573660850525, 0.9996150135993958, 0.9996086359024048, 0.9997419714927673, 0.9995461702346802, 0.9998391270637512, 0.9996274709701538, 0.9996533393859863, 0.9993970990180969, 0.9990370869636536, 0.9994319081306458, 0.9995775818824768, 0.999396026134491, 0.9993616938591003, 0.9999058842658997, 0.9993021488189697, 0.9998549222946167, 0.9995449781417847, 0.9998799562454224], \"Term\": [\"also\", \"amazing\", \"be\", \"beautiful\", \"bed\", \"buy\", \"case\", \"cheap\", \"color\", \"come\", \"comfortable\", \"cool\", \"corner\", \"cotton\", \"deep\", \"definitely\", \"even\", \"ever\", \"expect\", \"fabric\", \"far\", \"feel\", \"first\", \"fit\", \"fitted\", \"get\", \"go\", \"good\", \"great\", \"happy\", \"have\", \"have\", \"high\", \"hold\", \"hot\", \"keep\", \"last\", \"light\", \"little\", \"look\", \"love\", \"make\", \"material\", \"mattress\", \"money\", \"month\", \"much\", \"need\", \"new\", \"nice\", \"night\", \"order\", \"perfect\", \"pillow\", \"pocket\", \"pretty\", \"price\", \"product\", \"purchase\", \"put\", \"quality\", \"really\", \"recommend\", \"return\", \"review\", \"say\", \"see\", \"seem\", \"set\", \"sheet\", \"size\", \"sleep\", \"soft\", \"stain\", \"star\", \"stay\", \"still\", \"super\", \"sure\", \"thin\", \"think\", \"time\", \"top\", \"try\", \"use\", \"value\", \"want\", \"warm\", \"wash\", \"way\", \"well\", \"worth\", \"would\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1783112292508464371410415\", ldavis_el1783112292508464371410415_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1783112292508464371410415\", ldavis_el1783112292508464371410415_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1783112292508464371410415\", ldavis_el1783112292508464371410415_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2     -0.377071  0.018208       1        1  40.401203\n",
       "1      0.172599 -0.332211       2        1  31.653849\n",
       "0      0.204472  0.314003       3        1  27.944950, topic_info=       Term          Freq         Total Category  logprob  loglift\n",
       "80    sheet  40679.000000  40679.000000  Default  30.0000  30.0000\n",
       "5      soft  21413.000000  21413.000000  Default  29.0000  29.0000\n",
       "64    great  11052.000000  11052.000000  Default  28.0000  28.0000\n",
       "30     good   7728.000000   7728.000000  Default  27.0000  27.0000\n",
       "380   color   7760.000000   7760.000000  Default  26.0000  26.0000\n",
       "..      ...           ...           ...      ...      ...      ...\n",
       "175   money   1151.632935   1152.378418   Topic3  -4.7932   1.2743\n",
       "277  corner   1097.525513   1098.256714   Topic3  -4.8413   1.2743\n",
       "176   worth   1107.730103   1108.504395   Topic3  -4.8321   1.2742\n",
       "24     warm    949.883728    950.606750   Topic3  -4.9858   1.2742\n",
       "236    need   1034.984253   1035.784302   Topic3  -4.9000   1.2742\n",
       "\n",
       "[122 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "16        3  0.999690       also\n",
       "153       3  0.999391    amazing\n",
       "1069      2  0.999632         be\n",
       "161       3  0.999280  beautiful\n",
       "68        1  0.999899        bed\n",
       "...     ...       ...        ...\n",
       "166       2  0.999906       wash\n",
       "55        2  0.999302        way\n",
       "87        3  0.999855       well\n",
       "176       3  0.999545      worth\n",
       "46        2  0.999880      would\n",
       "\n",
       "[93 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Mallet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet' # update this path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('star', 0.14686502687813224),\n",
      "   ('great', 0.14118603550583067),\n",
      "   ('price', 0.09836304287153103),\n",
      "   ('good', 0.09235641738159667),\n",
      "   ('quality', 0.08411702605297965),\n",
      "   ('color', 0.07007729738256743),\n",
      "   ('nice', 0.046342025749614726),\n",
      "   ('product', 0.03833319176303559),\n",
      "   ('sheet', 0.019609508670169522),\n",
      "   ('amazing', 0.018857163659309055)]),\n",
      " (1,\n",
      "  [('soft', 0.23981194982130435),\n",
      "   ('love', 0.12989614065385816),\n",
      "   ('comfortable', 0.07073469094049076),\n",
      "   ('warm', 0.04252716413614449),\n",
      "   ('super', 0.038473531457628746),\n",
      "   ('sleep', 0.03373629800196685),\n",
      "   ('feel', 0.02958672135472884),\n",
      "   ('perfect', 0.022462881677100574),\n",
      "   ('light', 0.020555995298745532),\n",
      "   ('night', 0.020196205416037033)]),\n",
      " (2,\n",
      "  [('buy', 0.09898185251994528),\n",
      "   ('set', 0.09047105967240106),\n",
      "   ('wash', 0.05459982324669193),\n",
      "   ('sheet', 0.04742073340516459),\n",
      "   ('order', 0.04635537100035109),\n",
      "   ('time', 0.030217551845619302),\n",
      "   ('cover', 0.016125712763767994),\n",
      "   ('hold', 0.016065180808949045),\n",
      "   ('case', 0.014830328930642487),\n",
      "   ('duvet', 0.013656009007154878)]),\n",
      " (3,\n",
      "  [('sheet', 0.2391007763169015),\n",
      "   ('bed', 0.07632776751542371),\n",
      "   ('fit', 0.05924396633947867),\n",
      "   ('mattress', 0.03404686877467492),\n",
      "   ('recommend', 0.02648894684100595),\n",
      "   ('deep', 0.022214978207589314),\n",
      "   ('pillow', 0.021104229297209846),\n",
      "   ('size', 0.020428121264804956),\n",
      "   ('put', 0.017276974899489296),\n",
      "   ('pocket', 0.014850229997464595)]),\n",
      " (4,\n",
      "  [('sheet', 0.10078227811083491),\n",
      "   ('feel', 0.04714383194424136),\n",
      "   ('thin', 0.046802651459765074),\n",
      "   ('purchase', 0.03850465467660964),\n",
      "   ('comforter', 0.03433737875907784),\n",
      "   ('make', 0.03159574986596481),\n",
      "   ('expect', 0.022201101525564166),\n",
      "   ('happy', 0.020275868791733685),\n",
      "   ('review', 0.019045182044158504),\n",
      "   ('cheap', 0.018228785884875958)])]\n",
      "\n",
      "Coherence Score:  0.3841902362948132\n"
     ]
    }
   ],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=5, id2word=id2word)\n",
    "\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "# 0- quality of cooking, 1-good appliance, 2-functionalities 3-product design, 5-how likely to recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamallet_3topics = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=3, id2word=id2word)\n",
    "\n",
    "# # Show Topics\n",
    "# pprint(ldamallet_3topics.show_topics(formatted=False))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_ldamallet = CoherenceModel(model=ldamallet_3topics, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamallet_4topics = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=4, id2word=id2word)\n",
    "\n",
    "# # Show Topics\n",
    "# pprint(ldamallet_4topics.show_topics(formatted=False))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_ldamallet = CoherenceModel(model=ldamallet_4topics, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "# # 0 - great product, 1- features and fucntions, 2-review after few days, 3-things to cook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamallet_6topics = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=6, id2word=id2word)\n",
    "\n",
    "# # Show Topics\n",
    "# pprint(ldamallet_6topics.show_topics(formatted=False))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_ldamallet = CoherenceModel(model=ldamallet_6topics, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamallet_10topics = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)\n",
    "\n",
    "# # Show Topics\n",
    "# pprint(ldamallet_10topics.show_topics(formatted=False))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_ldamallet = CoherenceModel(model=ldamallet_10topics, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamallet_15topics = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=15, id2word=id2word)\n",
    "\n",
    "# # Show Topics\n",
    "# pprint(ldamallet_15topics.show_topics(formatted=False))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_ldamallet = CoherenceModel(model=ldamallet_15topics, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamallet_20topics = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
    "\n",
    "# # Show Topics\n",
    "# pprint(ldamallet_20topics.show_topics(formatted=False))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_ldamallet = CoherenceModel(model=ldamallet_20topics, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldamallet_25topics = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=25, id2word=id2word)\n",
    "\n",
    "# # Show Topics\n",
    "# pprint(ldamallet_25topics.show_topics(formatted=Tr))\n",
    "\n",
    "# # Compute Coherence Score\n",
    "# coherence_model_ldamallet = CoherenceModel(model=ldamallet_25topics, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "# coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "# print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding optimal number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=3, limit=25, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show graph\n",
    "limit=25; start=3; step=5;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Dominant Topic in each Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_lemmatized):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2671</td>\n",
       "      <td>soft, love, comfortable, warm, super, sleep, f...</td>\n",
       "      <td>[fine, comforter, super, soft, fluffy, hot, ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>star, great, price, good, quality, color, nice...</td>\n",
       "      <td>[especially, price, high, let, low, price, foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2243</td>\n",
       "      <td>sheet, feel, thin, purchase, comforter, make, ...</td>\n",
       "      <td>[compare, alternative, comforter, also]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>soft, love, comfortable, warm, super, sleep, f...</td>\n",
       "      <td>[difficult, expensive, clean, warm, cozy, light]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>buy, set, wash, sheet, order, time, cover, hol...</td>\n",
       "      <td>[see, hold, dry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134073</th>\n",
       "      <td>134073</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2401</td>\n",
       "      <td>soft, love, comfortable, warm, super, sleep, f...</td>\n",
       "      <td>[sheet, light, comfortable, feel, great, skin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134074</th>\n",
       "      <td>134074</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>buy, set, wash, sheet, order, time, cover, hol...</td>\n",
       "      <td>[heavy, sign, thick, thread, inflate]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134075</th>\n",
       "      <td>134075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>star, great, price, good, quality, color, nice...</td>\n",
       "      <td>[price, point, easy, order, several, complimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134076</th>\n",
       "      <td>134076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>buy, set, wash, sheet, order, time, cover, hol...</td>\n",
       "      <td>[order, additional, pillowcase, set]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134077</th>\n",
       "      <td>134077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2135</td>\n",
       "      <td>soft, love, comfortable, warm, super, sleep, f...</td>\n",
       "      <td>[wonderful]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134078 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0                 0             1.0              0.2671   \n",
       "1                 1             0.0              0.2542   \n",
       "2                 2             4.0              0.2243   \n",
       "3                 3             1.0              0.2500   \n",
       "4                 4             2.0              0.2308   \n",
       "...             ...             ...                 ...   \n",
       "134073       134073             1.0              0.2401   \n",
       "134074       134074             2.0              0.2182   \n",
       "134075       134075             0.0              0.2241   \n",
       "134076       134076             2.0              0.2407   \n",
       "134077       134077             1.0              0.2135   \n",
       "\n",
       "                                                 Keywords  \\\n",
       "0       soft, love, comfortable, warm, super, sleep, f...   \n",
       "1       star, great, price, good, quality, color, nice...   \n",
       "2       sheet, feel, thin, purchase, comforter, make, ...   \n",
       "3       soft, love, comfortable, warm, super, sleep, f...   \n",
       "4       buy, set, wash, sheet, order, time, cover, hol...   \n",
       "...                                                   ...   \n",
       "134073  soft, love, comfortable, warm, super, sleep, f...   \n",
       "134074  buy, set, wash, sheet, order, time, cover, hol...   \n",
       "134075  star, great, price, good, quality, color, nice...   \n",
       "134076  buy, set, wash, sheet, order, time, cover, hol...   \n",
       "134077  soft, love, comfortable, warm, super, sleep, f...   \n",
       "\n",
       "                                                     Text  \n",
       "0       [fine, comforter, super, soft, fluffy, hot, ni...  \n",
       "1       [especially, price, high, let, low, price, foo...  \n",
       "2                 [compare, alternative, comforter, also]  \n",
       "3        [difficult, expensive, clean, warm, cozy, light]  \n",
       "4                                        [see, hold, dry]  \n",
       "...                                                   ...  \n",
       "134073     [sheet, light, comfortable, feel, great, skin]  \n",
       "134074              [heavy, sign, thick, thread, inflate]  \n",
       "134075  [price, point, easy, order, several, complimen...  \n",
       "134076               [order, additional, pillowcase, set]  \n",
       "134077                                        [wonderful]  \n",
       "\n",
       "[134078 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis using Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "# Get the polarity score using below function\n",
    "def get_textBlob_score(sent):\n",
    "    # This polarity score is between -1 to 1\n",
    "    polarity = TextBlob(sent).sentiment.polarity\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134078"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_sentences_lemetised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sentiment scores for all sentences from text blob\n",
    "texblog_senti_scores= []\n",
    "for sentence in list_sentences_lemetised:\n",
    "    texblog_score = get_textBlob_score(sentence)\n",
    "    texblog_senti_scores.append(texblog_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134078"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texblog_senti_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.016666666666666666, 'crush old duvet bit dust put order long possible')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texblog_senti_scores[30000], list_sentences_lemetised[30000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Sentiment Analysis with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['topics'] = df_dominant_topic['Dominant_Topic'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['sentiment_scores'] = np.resize(texblog_senti_scores,len(df_full))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>rev_date</th>\n",
       "      <th>sentences</th>\n",
       "      <th>topics</th>\n",
       "      <th>sentiment_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>\\nI especially like the price was not high but...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>I am comparing it to both a 'down' and anothe...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>\"Down\" is difficult to clean and expensive as...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>We'll see how it holds up in the wash and dry</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136563</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136564</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>They are not too heavy, which is a sign that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136565</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>At this price point it was easy to order seve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136566</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>I ordered the additional pillowcases for each...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136567</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>//Wonderful</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136568 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall     reviewerID        asin  \\\n",
       "0           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "1           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "2           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "3           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "4           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "...         ...            ...         ...   \n",
       "136563      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136564      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136565      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136566      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136567      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "\n",
       "                                               reviewText   rev_date  \\\n",
       "0       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "1       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "2       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "3       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "4       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "...                                                   ...        ...   \n",
       "136563  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136564  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136565  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136566  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136567  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "\n",
       "                                                sentences  topics  \\\n",
       "0       This is a fine comforter, super soft and fluff...     1.0   \n",
       "1       \\nI especially like the price was not high but...     0.0   \n",
       "2        I am comparing it to both a 'down' and anothe...     4.0   \n",
       "3        \"Down\" is difficult to clean and expensive as...     1.0   \n",
       "4           We'll see how it holds up in the wash and dry     2.0   \n",
       "...                                                   ...     ...   \n",
       "136563  These sheets are light and comfortable! They f...     NaN   \n",
       "136564   They are not too heavy, which is a sign that ...     NaN   \n",
       "136565   At this price point it was easy to order seve...     NaN   \n",
       "136566   I ordered the additional pillowcases for each...     NaN   \n",
       "136567                                        //Wonderful     NaN   \n",
       "\n",
       "        sentiment_scores  \n",
       "0               0.180000  \n",
       "1               0.184000  \n",
       "2               0.000000  \n",
       "3               0.027778  \n",
       "4              -0.066667  \n",
       "...                  ...  \n",
       "136563         -0.075000  \n",
       "136564          0.011111  \n",
       "136565          0.000000  \n",
       "136566          0.000000  \n",
       "136567          0.000000  \n",
       "\n",
       "[136568 rows x 8 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    136568.000000\n",
       "mean          0.200552\n",
       "std           0.317961\n",
       "min          -1.000000\n",
       "25%           0.000000\n",
       "50%           0.100000\n",
       "75%           0.416667\n",
       "max           1.000000\n",
       "Name: sentiment_scores, dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full['sentiment_scores'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "texblog_senti_scores\n",
    "sentiments_textblob = []\n",
    "for each in texblog_senti_scores:\n",
    "    if(each >=0.4):\n",
    "        a=1\n",
    "    else:\n",
    "        a=0\n",
    "    sentiments_textblob.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['sentiment'] = np.resize(sentiments_textblob,len(df_full))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>rev_date</th>\n",
       "      <th>sentences</th>\n",
       "      <th>topics</th>\n",
       "      <th>sentiment_scores</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>\\nI especially like the price was not high but...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>I am comparing it to both a 'down' and anothe...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>\"Down\" is difficult to clean and expensive as...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AQDRWVTBXGNIZ</td>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>This is a fine comforter, super soft and fluff...</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>We'll see how it holds up in the wash and dry</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136563</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136564</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>They are not too heavy, which is a sign that ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136565</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>At this price point it was easy to order seve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136566</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>I ordered the additional pillowcases for each...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136567</th>\n",
       "      <td>5.0</td>\n",
       "      <td>AGLA12LDPFWU4</td>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>These sheets are light and comfortable! They f...</td>\n",
       "      <td>2018-08-12</td>\n",
       "      <td>//Wonderful</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136568 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall     reviewerID        asin  \\\n",
       "0           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "1           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "2           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "3           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "4           5.0  AQDRWVTBXGNIZ  B00635VODS   \n",
       "...         ...            ...         ...   \n",
       "136563      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136564      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136565      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136566      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "136567      5.0  AGLA12LDPFWU4  B00NLLUNSE   \n",
       "\n",
       "                                               reviewText   rev_date  \\\n",
       "0       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "1       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "2       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "3       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "4       This is a fine comforter, super soft and fluff... 2011-11-28   \n",
       "...                                                   ...        ...   \n",
       "136563  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136564  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136565  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136566  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "136567  These sheets are light and comfortable! They f... 2018-08-12   \n",
       "\n",
       "                                                sentences  topics  \\\n",
       "0       This is a fine comforter, super soft and fluff...     1.0   \n",
       "1       \\nI especially like the price was not high but...     0.0   \n",
       "2        I am comparing it to both a 'down' and anothe...     4.0   \n",
       "3        \"Down\" is difficult to clean and expensive as...     1.0   \n",
       "4           We'll see how it holds up in the wash and dry     2.0   \n",
       "...                                                   ...     ...   \n",
       "136563  These sheets are light and comfortable! They f...     NaN   \n",
       "136564   They are not too heavy, which is a sign that ...     NaN   \n",
       "136565   At this price point it was easy to order seve...     NaN   \n",
       "136566   I ordered the additional pillowcases for each...     NaN   \n",
       "136567                                        //Wonderful     NaN   \n",
       "\n",
       "        sentiment_scores  sentiment  \n",
       "0               0.180000          0  \n",
       "1               0.184000          0  \n",
       "2               0.000000          0  \n",
       "3               0.027778          0  \n",
       "4              -0.066667          0  \n",
       "...                  ...        ...  \n",
       "136563         -0.075000          0  \n",
       "136564          0.011111          0  \n",
       "136565          0.000000          0  \n",
       "136566          0.000000          0  \n",
       "136567          0.000000          0  \n",
       "\n",
       "[136568 rows x 9 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    54380\n",
       "1.0    25990\n",
       "2.0    20706\n",
       "4.0    17385\n",
       "3.0    15617\n",
       "Name: topics, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_full['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin        topics  sentiment\n",
       "B00635VODS  0.0     0             6887\n",
       "                    1             3231\n",
       "            1.0     0             3168\n",
       "                    1             2060\n",
       "            2.0     0             3228\n",
       "                    1              672\n",
       "            3.0     0             2225\n",
       "                    1              727\n",
       "            4.0     0             2779\n",
       "                    1              788\n",
       "B00902X68W  0.0     0            10452\n",
       "                    1             4918\n",
       "            1.0     0             4754\n",
       "                    1             3090\n",
       "            2.0     0             4926\n",
       "                    1             1090\n",
       "            3.0     0             3225\n",
       "                    1              987\n",
       "            4.0     0             4080\n",
       "                    1             1227\n",
       "B00LV4W8BI  0.0     0            12680\n",
       "                    1             5857\n",
       "            1.0     0             5197\n",
       "                    1             2908\n",
       "            2.0     0             5459\n",
       "                    1             1107\n",
       "            3.0     0             3849\n",
       "                    1             1280\n",
       "            4.0     0             4144\n",
       "                    1             1227\n",
       "B00NLLUNSE  0.0     0             7183\n",
       "                    1             3172\n",
       "            1.0     0             3254\n",
       "                    1             1559\n",
       "            2.0     0             3523\n",
       "                    1              701\n",
       "            3.0     0             2532\n",
       "                    1              792\n",
       "            4.0     0             2424\n",
       "                    1              716\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_wise_sentiment_counts = df_full.groupby(['asin','topics'])['sentiment'].value_counts()\n",
    "topic_wise_sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "asin        topics\n",
       "B00635VODS  0.0       10118\n",
       "            1.0        5228\n",
       "            2.0        3900\n",
       "            3.0        2952\n",
       "            4.0        3567\n",
       "B00902X68W  0.0       15370\n",
       "            1.0        7844\n",
       "            2.0        6016\n",
       "            3.0        4212\n",
       "            4.0        5307\n",
       "B00LV4W8BI  0.0       18537\n",
       "            1.0        8105\n",
       "            2.0        6566\n",
       "            3.0        5129\n",
       "            4.0        5371\n",
       "B00NLLUNSE  0.0       10355\n",
       "            1.0        4813\n",
       "            2.0        4224\n",
       "            3.0        3324\n",
       "            4.0        3140\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sentiment = df_full.groupby(['asin','topics'])['sentiment'].count()\n",
    "total_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10118, 6887)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sentiment.loc['B00635VODS'][0], topic_wise_sentiment_counts['B00635VODS'][0][0]\n",
    "# , topic_wise_sentiment_counts[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6806681162285037,\n",
       "  0.6059678653404744,\n",
       "  0.8276923076923077,\n",
       "  0.7537262872628726,\n",
       "  0.7790860667227362,\n",
       "  0.6800260247234873,\n",
       "  0.6060683324834268,\n",
       "  0.8188164893617021,\n",
       "  0.7656695156695157,\n",
       "  0.7687959299039006,\n",
       "  0.6840373307439176,\n",
       "  0.6412091301665639,\n",
       "  0.8314042034724337,\n",
       "  0.7504386820042893,\n",
       "  0.7715509216160864,\n",
       "  0.6936745533558667,\n",
       "  0.6760856014959484,\n",
       "  0.8340435606060606,\n",
       "  0.7617328519855595,\n",
       "  0.7719745222929937],\n",
       " [3, 3, 5, 5, 5, 3, 3, 5, 5, 5, 3, 3, 5, 5, 5, 3, 3, 5, 5, 5])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "ratings= []\n",
    "product_ids = ['B00635VODS', 'B00902X68W', 'B00LV4W8BI', 'B00NLLUNSE']\n",
    "for i in range(len(product_ids)):\n",
    "    for j in range(5):\n",
    "        value = topic_wise_sentiment_counts[product_ids[i]][j][0]/total_sentiment.loc[product_ids[i]][j]\n",
    "        results.append(value)\n",
    "        if (value >0.75):\n",
    "            ratings.append(5)\n",
    "        elif (value >0.7):\n",
    "            ratings.append(4)\n",
    "        elif (value >0.5):\n",
    "            ratings.append(3)\n",
    "        elif (value >0.4):\n",
    "            ratings.append(2)\n",
    "        else: ratings.append(1)\n",
    "results, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 5, 5, 5], [5, 5, 5, 5], [3, 2, 3, 3], [4, 4, 4, 5], [4, 4, 5, 5]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_ids_tmp =  [5, 5, 3, 4, 4, 5, 5, 2, 4, 4, 5, 5, 3, 4, 5, 5, 5, 3, 5, 5]\n",
    "product_id_topics = [[],[],[],[],[]]\n",
    "for i in range(len(product_ids_tmp)):\n",
    "    product_id_topics[int(i%5)].append(product_ids_tmp[i])\n",
    "product_id_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], []]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].append(1)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic wise product rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['Softnest and comfort', 'Fit', 'Quality', 'Recommendation', 'After wash quality']\n",
    "products = {'comforter':'B00635VODS','sheets-1':'B00902X68W',\n",
    "           'sheets-2':'B00LV4W8BI',\n",
    "           'sheets-3': 'B00NLLUNSE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productID</th>\n",
       "      <th>Softnest and comfort</th>\n",
       "      <th>Fit</th>\n",
       "      <th>Quality</th>\n",
       "      <th>Recommendation</th>\n",
       "      <th>After wash quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00635VODS</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00902X68W</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00LV4W8BI</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00NLLUNSE</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productID  Softnest and comfort  Fit  Quality  Recommendation  \\\n",
       "0  B00635VODS                     5    5        3               4   \n",
       "1  B00902X68W                     5    5        2               4   \n",
       "2  B00LV4W8BI                     5    5        3               4   \n",
       "3  B00NLLUNSE                     5    5        3               5   \n",
       "\n",
       "   After wash quality  \n",
       "0                   4  \n",
       "1                   4  \n",
       "2                   5  \n",
       "3                   5  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Product_Summary = pd.DataFrame({'productID':product_ids,\n",
    "                                topics[0]: product_id_topics[0],\n",
    "                                topics[1]: product_id_topics[1],\n",
    "                                topics[2]: product_id_topics[2],\n",
    "                                topics[3]: product_id_topics[3],\n",
    "                                topics[4]: product_id_topics[4]})\n",
    "Product_Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Wise Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sentimental_sentences(n, sentiment, sentences):\n",
    "    print(\"\\033[1m\"+ \"Most \"+ sentiment+\" sentences are: \" + \"\\033[0m\")\n",
    "    print(\"\")\n",
    "    for i in range(len(sentences.index)):\n",
    "        print(i,\") \",df_full.iloc[sentences.index[i]]['sentences'])\n",
    "\n",
    "def get_product_summary(product_name):\n",
    "    product_id = products[product_name]\n",
    "    product_df = df_full[df_full['asin']==product_id]\n",
    "    print(\"\\033[1m\" + \"Product summary for \"+ product_name + \"\\033[0m\")\n",
    "    print(\"\")\n",
    "    print(\"\\033[1m\" + \"Topic wise rating is...\"+ \"\\033[0m\")\n",
    "    print(\"\")\n",
    "    product_rating = Product_Summary[Product_Summary['productID']==product_id]\n",
    "    product_rating = product_rating.drop(['productID'], axis=1)\n",
    "    print(product_rating.T)\n",
    "    print(\"\")\n",
    "    Most_negative_sentences =  product_df.nsmallest(5, 'sentiment_scores')['sentences']\n",
    "    Most_positive_sentences = product_df.nlargest(5, 'sentiment_scores')['sentences']\n",
    "    get_n_sentimental_sentences(5, \"positive\", Most_positive_sentences )\n",
    "    get_n_sentimental_sentences(5, \"negative\", Most_negative_sentences )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mProduct summary for comforter\u001b[0m\n",
      "\n",
      "\u001b[1mTopic wise rating is...\u001b[0m\n",
      "\n",
      "                      0\n",
      "Softnest and comfort  5\n",
      "Fit                   5\n",
      "Quality               3\n",
      "Recommendation        4\n",
      "After wash quality    4\n",
      "\n",
      "\u001b[1mMost positive sentences are: \u001b[0m\n",
      "\n",
      "0 )   It would be perfect for a Queen sized bed\n",
      "1 )  nothing overly wonderful\n",
      "2 )  This is an excellent duvet for people allergic to feathers\n",
      "3 )    It looks wonderful with my duvet cover, doesn't 'pill' and the stitching keeps it from bunching\n",
      "4 )  This duvet was the perfect weight\n",
      "\u001b[1mMost negative sentences are: \u001b[0m\n",
      "\n",
      "0 )   Shipping issues aside (first one went AWOL, they shipped another without incident), I wish I'd never ordered it\n",
      "1 )  Feels great even in the Texas weather\n",
      "2 )    It barely covers the top of the bed\n",
      "3 )   //great buy!\n",
      "4 )  True size, I expected slightly thicker but it is a good price/ quality \n"
     ]
    }
   ],
   "source": [
    "# Product name can be given here which fetches productID from the dictionary created above\n",
    "get_product_summary(\"comforter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
